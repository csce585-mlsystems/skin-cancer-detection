{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-29T04:42:46.936615Z",
     "iopub.status.busy": "2024-11-29T04:42:46.936363Z",
     "iopub.status.idle": "2024-11-29T04:42:56.686666Z",
     "shell.execute_reply": "2024-11-29T04:42:56.685757Z",
     "shell.execute_reply.started": "2024-11-29T04:42:46.936561Z"
    }
   },
   "outputs": [],
   "source": [
    "!pip install torchsummary\n",
    "!pip install pretrainedmodels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-29T04:42:56.688878Z",
     "iopub.status.busy": "2024-11-29T04:42:56.688599Z",
     "iopub.status.idle": "2024-11-29T04:42:58.840319Z",
     "shell.execute_reply": "2024-11-29T04:42:58.839595Z",
     "shell.execute_reply.started": "2024-11-29T04:42:56.688850Z"
    }
   },
   "outputs": [],
   "source": [
    "# import for train.py\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "import torchvision\n",
    "from torchvision import datasets, models as tv_models\n",
    "from torch.utils.data import DataLoader\n",
    "from torchsummary import summary\n",
    "import numpy as np\n",
    "from scipy import io\n",
    "import threading\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "import math\n",
    "import os\n",
    "import sys\n",
    "from glob import glob\n",
    "import re\n",
    "import gc\n",
    "import importlib\n",
    "import time\n",
    "import sklearn.preprocessing\n",
    "from sklearn.utils import class_weight\n",
    "import psutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install efficientnet_pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-29T04:43:18.049772Z",
     "iopub.status.busy": "2024-11-29T04:43:18.049481Z",
     "iopub.status.idle": "2024-11-29T04:43:19.594439Z",
     "shell.execute_reply": "2024-11-29T04:43:19.593431Z",
     "shell.execute_reply.started": "2024-11-29T04:43:18.049742Z"
    }
   },
   "outputs": [],
   "source": [
    "# import for models.py\n",
    "import torch\n",
    "import numbers\n",
    "import numpy as np\n",
    "import functools\n",
    "import h5py\n",
    "import math\n",
    "from torchvision import models\n",
    "import pretrainedmodels\n",
    "import torch.nn.functional as F\n",
    "import types\n",
    "import torch\n",
    "from efficientnet_pytorch import EfficientNet\n",
    "from collections import OrderedDict\n",
    "import torch.nn as nn\n",
    "\n",
    "def Dense121(config):\n",
    "    return models.densenet121(pretrained=True)\n",
    "\n",
    "def Dense161(config):\n",
    "    return models.densenet169(pretrained=True)\n",
    "\n",
    "def Dense169(config):\n",
    "    return models.densenet161(pretrained=True)\n",
    "\n",
    "def Dense201(config):\n",
    "    return models.densenet201(pretrained=True)\n",
    "\n",
    "def Resnet50(config):\n",
    "    return pretrainedmodels.__dict__['resnet50'](num_classes=1000, pretrained='imagenet')\n",
    "\n",
    "def Resnet101(config):\n",
    "    return models.resnet101(pretrained=True)\n",
    "\n",
    "def InceptionV3(config):\n",
    "    return models.inception_v3(pretrained=True)\n",
    "\n",
    "def se_resnext50(config):\n",
    "    return pretrainedmodels.__dict__['se_resnext50_32x4d'](num_classes=1000, pretrained='imagenet')\n",
    "\n",
    "def se_resnext101(config):\n",
    "    return pretrainedmodels.__dict__['se_resnext101_32x4d'](num_classes=1000, pretrained='imagenet')\n",
    "\n",
    "def se_resnet50(config):\n",
    "    return pretrainedmodels.__dict__['se_resnet50'](num_classes=1000, pretrained='imagenet')\n",
    "\n",
    "def se_resnet101(config):\n",
    "    return pretrainedmodels.__dict__['se_resnet101'](num_classes=1000, pretrained='imagenet')\n",
    "\n",
    "def se_resnet152(config):\n",
    "    return pretrainedmodels.__dict__['se_resnet152'](num_classes=1000, pretrained='imagenet')\n",
    "\n",
    "def resnext101(config):\n",
    "    return pretrainedmodels.__dict__['resnext101_32x4d'](num_classes=1000, pretrained='imagenet')\n",
    "\n",
    "def resnext101_64(config):\n",
    "    return pretrainedmodels.__dict__['resnext101_64x4d'](num_classes=1000, pretrained='imagenet')\n",
    "\n",
    "def senet154(config):\n",
    "    return pretrainedmodels.__dict__['senet154'](num_classes=1000, pretrained='imagenet')\n",
    "\n",
    "def polynet(config):\n",
    "    return pretrainedmodels.__dict__['polynet'](num_classes=1000, pretrained='imagenet')\n",
    "\n",
    "def dpn92(config):\n",
    "    return pretrainedmodels.__dict__['dpn92'](num_classes=1000, pretrained='imagenet+5k')\n",
    "\n",
    "def dpn68b(config):\n",
    "    return pretrainedmodels.__dict__['dpn68b'](num_classes=1000, pretrained='imagenet+5k')\n",
    "\n",
    "def nasnetamobile(config):\n",
    "    return pretrainedmodels.__dict__['nasnetamobile'](num_classes=1000, pretrained='imagenet')\n",
    "\n",
    "def resnext101_32_8_wsl(config):\n",
    "    return torch.hub.load('facebookresearch/WSL-Images', 'resnext101_32x8d_wsl')\n",
    "\n",
    "def resnext101_32_16_wsl(config):\n",
    "    return torch.hub.load('facebookresearch/WSL-Images', 'resnext101_32x16d_wsl')\n",
    "\n",
    "def resnext101_32_32_wsl(config):\n",
    "    return torch.hub.load('facebookresearch/WSL-Images', 'resnext101_32x32d_wsl')\n",
    "\n",
    "def resnext101_32_48_wsl(config):\n",
    "    return torch.hub.load('facebookresearch/WSL-Images', 'resnext101_32x48d_wsl')\n",
    "\n",
    "def efficientnet_b0(config):\n",
    "    return EfficientNet.from_pretrained('efficientnet-b0',num_classes=config['numClasses'])\n",
    "\n",
    "def efficientnet_b1(config):\n",
    "    return EfficientNet.from_pretrained('efficientnet-b1',num_classes=config['numClasses'])\n",
    "\n",
    "def efficientnet_b2(config):\n",
    "    return EfficientNet.from_pretrained('efficientnet-b2',num_classes=config['numClasses'])\n",
    "\n",
    "def efficientnet_b3(config):\n",
    "    return EfficientNet.from_pretrained('efficientnet-b3',num_classes=config['numClasses'])\n",
    "\n",
    "def efficientnet_b4(config):\n",
    "    return EfficientNet.from_pretrained('efficientnet-b4',num_classes=config['numClasses'])\n",
    "\n",
    "def efficientnet_b5(config):\n",
    "    return EfficientNet.from_pretrained('efficientnet-b5',num_classes=config['numClasses'])       \n",
    "\n",
    "def efficientnet_b6(config):\n",
    "    return EfficientNet.from_pretrained('efficientnet-b6',num_classes=config['numClasses'])   \n",
    "\n",
    "def efficientnet_b7(config):\n",
    "    return EfficientNet.from_pretrained('efficientnet-b7',num_classes=config['numClasses'])  \n",
    "\n",
    "def isic2019_efficientnet_b0(config):\n",
    "    # Load the EfficientNet B0 model with the specified number of output classes\n",
    "    model = EfficientNet.from_pretrained('efficientnet-b0', num_classes=config['numClasses'])\n",
    "    \n",
    "    # Load the state dict from the provided path and update the model\n",
    "    model.load_state_dict(torch.load('/kaggle/input/extrafiles/checkpoint_best-60.pt')['state_dict'])\n",
    "    \n",
    "    # Freeze specific layers (set requires_grad=False for frozen layers)\n",
    "    for name, param in model.named_parameters():\n",
    "        if any(layer in name for layer in ['_conv_stem']):\n",
    "            param.requires_grad = False\n",
    "        else:\n",
    "            param.requires_grad = True\n",
    "\n",
    "    return model\n",
    "    \n",
    "def modify_meta(mdlParams,model):\n",
    "    # Define FC layers\n",
    "    if len(mdlParams['fc_layers_before']) > 1:\n",
    "        model.meta_before = nn.Sequential(nn.Linear(mdlParams['meta_array'].shape[1],mdlParams['fc_layers_before'][0]),\n",
    "                                    nn.BatchNorm1d(mdlParams['fc_layers_before'][0]),\n",
    "                                    nn.ReLU(),\n",
    "                                    nn.Dropout(p=mdlParams['dropout_meta']),\n",
    "                                    nn.Linear(mdlParams['fc_layers_before'][0],mdlParams['fc_layers_before'][1]),\n",
    "                                    nn.BatchNorm1d(mdlParams['fc_layers_before'][1]),\n",
    "                                    nn.ReLU(),\n",
    "                                    nn.Dropout(p=mdlParams['dropout_meta']))\n",
    "    else:\n",
    "        model.meta_before = nn.Sequential(nn.Linear(mdlParams['meta_array'].shape[1],mdlParams['fc_layers_before'][0]),\n",
    "                                    nn.BatchNorm1d(mdlParams['fc_layers_before'][0]),\n",
    "                                    nn.ReLU(),\n",
    "                                    nn.Dropout(p=mdlParams['dropout_meta']))\n",
    "    # Define fc layers after\n",
    "    if len(mdlParams['fc_layers_after']) > 0:\n",
    "        if 'efficient' in mdlParams['model_type']:\n",
    "            num_cnn_features = model._fc.in_features \n",
    "        elif 'wsl' in mdlParams['model_type']:\n",
    "            num_cnn_features = model.fc.in_features  \n",
    "        else:\n",
    "            num_cnn_features = model.last_linear.in_features     \n",
    "        model.meta_after = nn.Sequential(nn.Linear(mdlParams['fc_layers_before'][-1]+num_cnn_features,mdlParams['fc_layers_after'][0]),\n",
    "                                    nn.BatchNorm1d(mdlParams['fc_layers_after'][0]),\n",
    "                                    nn.ReLU())\n",
    "        classifier_in_features = mdlParams['fc_layers_after'][0] \n",
    "    else:\n",
    "        model.meta_after = None\n",
    "        classifier_in_features = mdlParams['fc_layers_before'][-1]+model._fc.in_features\n",
    "    # Modify classifier\n",
    "    if 'efficient' in mdlParams['model_type']:\n",
    "        model._fc = nn.Linear(classifier_in_features, mdlParams['numClasses'])\n",
    "    elif 'wsl' in mdlParams['model_type']:\n",
    "        model.fc = nn.Linear(classifier_in_features, mdlParams['numClasses']) \n",
    "    else:\n",
    "        model.last_linear = nn.Linear(classifier_in_features, mdlParams['numClasses'])       \n",
    "    # Modify forward pass\n",
    "    def new_forward(self, inputs):\n",
    "        x, meta_data = inputs\n",
    "        # Normal CNN features\n",
    "        if 'efficient' in mdlParams['model_type']:\n",
    "            # Convolution layers\n",
    "            cnn_features = self.extract_features(x)\n",
    "            # Pooling and final linear layer\n",
    "            cnn_features = F.adaptive_avg_pool2d(cnn_features, 1).squeeze(-1).squeeze(-1)\n",
    "            if self._dropout:\n",
    "                cnn_features = F.dropout(cnn_features, p=self._dropout, training=self.training)\n",
    "        elif 'wsl' in mdlParams['model_type']:\n",
    "            cnn_features = self.conv1(x)\n",
    "            cnn_features = self.bn1(cnn_features)\n",
    "            cnn_features = self.relu(cnn_features)\n",
    "            cnn_features = self.maxpool(cnn_features)\n",
    "\n",
    "            cnn_features = self.layer1(cnn_features)\n",
    "            cnn_features = self.layer2(cnn_features)\n",
    "            cnn_features = self.layer3(cnn_features)\n",
    "            cnn_features = self.layer4(cnn_features)\n",
    "\n",
    "            cnn_features = self.avgpool(cnn_features)\n",
    "            cnn_features = torch.flatten(cnn_features, 1) \n",
    "        else:\n",
    "            cnn_features = self.layer0(x)\n",
    "            cnn_features = self.layer1(cnn_features)\n",
    "            cnn_features = self.layer2(cnn_features)\n",
    "            cnn_features = self.layer3(cnn_features)\n",
    "            cnn_features = self.layer4(cnn_features)   \n",
    "            cnn_features = self.avg_pool(cnn_features)\n",
    "            if self.dropout is not None:\n",
    "                cnn_features = self.dropout(cnn_features)\n",
    "            cnn_features = cnn_features.view(cnn_features.size(0), -1)                                \n",
    "        # Meta part\n",
    "        #print(meta_data.shape,meta_data)\n",
    "        meta_features = self.meta_before(meta_data)\n",
    "\n",
    "        # Cat\n",
    "        features = torch.cat((cnn_features,meta_features),dim=1)\n",
    "        #print(\"features cat\",features.shape)\n",
    "        if self.meta_after is not None:\n",
    "            features = self.meta_after(features)\n",
    "        # Classifier\n",
    "        if 'efficient' in mdlParams['model_type']:\n",
    "            output = self._fc(features)\n",
    "        elif 'wsl' in mdlParams['model_type']:\n",
    "            output = self.fc(features)\n",
    "        else:\n",
    "            output = self.last_linear(features)\n",
    "        return output\n",
    "    model.forward  = types.MethodType(new_forward, model)\n",
    "    return model                                                                                                                       \n",
    "\n",
    "model_map = OrderedDict([('Dense121',  Dense121),\n",
    "                        ('Dense169' , Dense161),\n",
    "                        ('Dense161' , Dense169),\n",
    "                        ('Dense201' , Dense201),\n",
    "                        ('Resnet50' , Resnet50),\n",
    "                        ('Resnet101' , Resnet101),   \n",
    "                        ('InceptionV3', InceptionV3),# models.inception_v3(pretrained=True),\n",
    "                        ('se_resnext50', se_resnext50),\n",
    "                        ('se_resnext101', se_resnext101),\n",
    "                        ('se_resnet50', se_resnet50),\n",
    "                        ('se_resnet101', se_resnet101),\n",
    "                        ('se_resnet152', se_resnet152),\n",
    "                        ('resnext101', resnext101),\n",
    "                        ('resnext101_64', resnext101_64),\n",
    "                        ('senet154', senet154),\n",
    "                        ('polynet', polynet),\n",
    "                        ('dpn92', dpn92),\n",
    "                        ('dpn68b', dpn68b),\n",
    "                        ('nasnetamobile', nasnetamobile),\n",
    "                        ('resnext101_32_8_wsl', resnext101_32_8_wsl),\n",
    "                        ('resnext101_32_16_wsl', resnext101_32_16_wsl),\n",
    "                        ('resnext101_32_32_wsl', resnext101_32_32_wsl),\n",
    "                        ('resnext101_32_48_wsl', resnext101_32_48_wsl),\n",
    "                        ('efficientnet-b0', efficientnet_b0), \n",
    "                        ('efficientnet-b1', efficientnet_b1), \n",
    "                        ('efficientnet-b2', efficientnet_b2), \n",
    "                        ('efficientnet-b3', efficientnet_b3),  \n",
    "                        ('efficientnet-b4', efficientnet_b4), \n",
    "                        ('efficientnet-b5', efficientnet_b5),  \n",
    "                        ('efficientnet-b6', efficientnet_b6), \n",
    "                        ('efficientnet-b7', efficientnet_b7),\n",
    "                        ('isic2019-efficientnet-b0', isic2019_efficientnet_b0)\n",
    "                    ])\n",
    "\n",
    "def getModel(config):\n",
    "  \"\"\"Returns a function for a model\n",
    "  Args:\n",
    "    config: dictionary, contains configuration\n",
    "  Returns:\n",
    "    model: A class that builds the desired model\n",
    "  Raises:\n",
    "    ValueError: If model name is not recognized.\n",
    "  \"\"\"\n",
    "  if config['model_type'] in model_map:\n",
    "    func = model_map[config['model_type'] ]\n",
    "    @functools.wraps(func)\n",
    "    def model():\n",
    "        return func(config)\n",
    "  else:\n",
    "      raise ValueError('Name of model unknown %s' % config['model_name'] )\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-29T04:44:57.076287Z",
     "iopub.status.busy": "2024-11-29T04:44:57.075778Z",
     "iopub.status.idle": "2024-11-29T04:44:57.138538Z",
     "shell.execute_reply": "2024-11-29T04:44:57.137621Z",
     "shell.execute_reply.started": "2024-11-29T04:44:57.076231Z"
    }
   },
   "outputs": [],
   "source": [
    "# import for autoaugment.py\n",
    "import random\n",
    "import numpy as np\n",
    "import scipy\n",
    "from scipy import ndimage\n",
    "from PIL import Image, ImageEnhance, ImageOps\n",
    "\n",
    "#See: https://github.com/4uiiurz1/pytorch-auto-augment\n",
    "class AutoAugment(object):\n",
    "    def __init__(self):\n",
    "        self.policies = [\n",
    "            ['Invert', 0.1, 7, 'Contrast', 0.2, 6],\n",
    "            ['Rotate', 0.7, 2, 'TranslateX', 0.3, 9],\n",
    "            ['Sharpness', 0.8, 1, 'Sharpness', 0.9, 3],\n",
    "            ['ShearY', 0.5, 8, 'TranslateY', 0.7, 9],\n",
    "            ['AutoContrast', 0.5, 8, 'Equalize', 0.9, 2],\n",
    "            ['ShearY', 0.2, 7, 'Posterize', 0.3, 7],\n",
    "            ['Color', 0.4, 3, 'Brightness', 0.6, 7],\n",
    "            ['Sharpness', 0.3, 9, 'Brightness', 0.7, 9],\n",
    "            ['Equalize', 0.6, 5, 'Equalize', 0.5, 1],\n",
    "            ['Contrast', 0.6, 7, 'Sharpness', 0.6, 5],\n",
    "            ['Color', 0.7, 7, 'TranslateX', 0.5, 8],\n",
    "            ['Equalize', 0.3, 7, 'AutoContrast', 0.4, 8],\n",
    "            ['TranslateY', 0.4, 3, 'Sharpness', 0.2, 6],\n",
    "            ['Brightness', 0.9, 6, 'Color', 0.2, 8],\n",
    "            ['Solarize', 0.5, 2, 'Invert', 0, 0.3],\n",
    "            ['Equalize', 0.2, 0, 'AutoContrast', 0.6, 0],\n",
    "            ['Equalize', 0.2, 8, 'Equalize', 0.6, 4],\n",
    "            ['Color', 0.9, 9, 'Equalize', 0.6, 6],\n",
    "            ['AutoContrast', 0.8, 4, 'Solarize', 0.2, 8],\n",
    "            ['Brightness', 0.1, 3, 'Color', 0.7, 0],\n",
    "            ['Solarize', 0.4, 5, 'AutoContrast', 0.9, 3],\n",
    "            ['TranslateY', 0.9, 9, 'TranslateY', 0.7, 9],\n",
    "            ['AutoContrast', 0.9, 2, 'Solarize', 0.8, 3],\n",
    "            ['Equalize', 0.8, 8, 'Invert', 0.1, 3],\n",
    "            ['TranslateY', 0.7, 9, 'AutoContrast', 0.9, 1],\n",
    "        ]\n",
    "\n",
    "    def __call__(self, img):\n",
    "        img = apply_policy(img, self.policies[random.randrange(len(self.policies))])\n",
    "        return img\n",
    "\n",
    "\n",
    "operations = {\n",
    "    'ShearX': lambda img, magnitude: shear_x(img, magnitude),\n",
    "    'ShearY': lambda img, magnitude: shear_y(img, magnitude),\n",
    "    'TranslateX': lambda img, magnitude: translate_x(img, magnitude),\n",
    "    'TranslateY': lambda img, magnitude: translate_y(img, magnitude),\n",
    "    'Rotate': lambda img, magnitude: rotate(img, magnitude),\n",
    "    'AutoContrast': lambda img, magnitude: auto_contrast(img, magnitude),\n",
    "    'Invert': lambda img, magnitude: invert(img, magnitude),\n",
    "    'Equalize': lambda img, magnitude: equalize(img, magnitude),\n",
    "    'Solarize': lambda img, magnitude: solarize(img, magnitude),\n",
    "    'Posterize': lambda img, magnitude: posterize(img, magnitude),\n",
    "    'Contrast': lambda img, magnitude: contrast(img, magnitude),\n",
    "    'Color': lambda img, magnitude: color(img, magnitude),\n",
    "    'Brightness': lambda img, magnitude: brightness(img, magnitude),\n",
    "    'Sharpness': lambda img, magnitude: sharpness(img, magnitude),\n",
    "    'Cutout': lambda img, magnitude: cutout(img, magnitude),\n",
    "}\n",
    "\n",
    "\n",
    "def apply_policy(img, policy):\n",
    "    if random.random() < policy[1]:\n",
    "        img = operations[policy[0]](img, policy[2])\n",
    "    if random.random() < policy[4]:\n",
    "        img = operations[policy[3]](img, policy[5])\n",
    "\n",
    "    return img\n",
    "\n",
    "\n",
    "def transform_matrix_offset_center(matrix, x, y):\n",
    "    o_x = float(x) / 2 + 0.5\n",
    "    o_y = float(y) / 2 + 0.5\n",
    "    offset_matrix = np.array([[1, 0, o_x], [0, 1, o_y], [0, 0, 1]])\n",
    "    reset_matrix = np.array([[1, 0, -o_x], [0, 1, -o_y], [0, 0, 1]])\n",
    "    transform_matrix = offset_matrix @ matrix @ reset_matrix\n",
    "    return transform_matrix\n",
    "\n",
    "\n",
    "def shear_x(img, magnitude):\n",
    "    img = np.array(img)\n",
    "    magnitudes = np.linspace(-0.3, 0.3, 11)\n",
    "\n",
    "    transform_matrix = np.array([[1, random.uniform(magnitudes[magnitude], magnitudes[magnitude+1]), 0],\n",
    "                                 [0, 1, 0],\n",
    "                                 [0, 0, 1]])\n",
    "    transform_matrix = transform_matrix_offset_center(transform_matrix, img.shape[0], img.shape[1])\n",
    "    affine_matrix = transform_matrix[:2, :2]\n",
    "    offset = transform_matrix[:2, 2]\n",
    "    img = np.stack([ndimage.interpolation.affine_transform(\n",
    "                    img[:, :, c],\n",
    "                    affine_matrix,\n",
    "                    offset) for c in range(img.shape[2])], axis=2)\n",
    "    img = Image.fromarray(img)\n",
    "    return img\n",
    "\n",
    "\n",
    "def shear_y(img, magnitude):\n",
    "    img = np.array(img)\n",
    "    magnitudes = np.linspace(-0.3, 0.3, 11)\n",
    "\n",
    "    transform_matrix = np.array([[1, 0, 0],\n",
    "                                 [random.uniform(magnitudes[magnitude], magnitudes[magnitude+1]), 1, 0],\n",
    "                                 [0, 0, 1]])\n",
    "    transform_matrix = transform_matrix_offset_center(transform_matrix, img.shape[0], img.shape[1])\n",
    "    affine_matrix = transform_matrix[:2, :2]\n",
    "    offset = transform_matrix[:2, 2]\n",
    "    img = np.stack([ndimage.interpolation.affine_transform(\n",
    "                    img[:, :, c],\n",
    "                    affine_matrix,\n",
    "                    offset) for c in range(img.shape[2])], axis=2)\n",
    "    img = Image.fromarray(img)\n",
    "    return img\n",
    "\n",
    "\n",
    "def translate_x(img, magnitude):\n",
    "    img = np.array(img)\n",
    "    magnitudes = np.linspace(-150/331, 150/331, 11)\n",
    "\n",
    "    transform_matrix = np.array([[1, 0, 0],\n",
    "                                 [0, 1, img.shape[1]*random.uniform(magnitudes[magnitude], magnitudes[magnitude+1])],\n",
    "                                 [0, 0, 1]])\n",
    "    transform_matrix = transform_matrix_offset_center(transform_matrix, img.shape[0], img.shape[1])\n",
    "    affine_matrix = transform_matrix[:2, :2]\n",
    "    offset = transform_matrix[:2, 2]\n",
    "    img = np.stack([ndimage.interpolation.affine_transform(\n",
    "                    img[:, :, c],\n",
    "                    affine_matrix,\n",
    "                    offset) for c in range(img.shape[2])], axis=2)\n",
    "    img = Image.fromarray(img)\n",
    "    return img\n",
    "\n",
    "\n",
    "def translate_y(img, magnitude):\n",
    "    img = np.array(img)\n",
    "    magnitudes = np.linspace(-150/331, 150/331, 11)\n",
    "\n",
    "    transform_matrix = np.array([[1, 0, img.shape[0]*random.uniform(magnitudes[magnitude], magnitudes[magnitude+1])],\n",
    "                                 [0, 1, 0],\n",
    "                                 [0, 0, 1]])\n",
    "    transform_matrix = transform_matrix_offset_center(transform_matrix, img.shape[0], img.shape[1])\n",
    "    affine_matrix = transform_matrix[:2, :2]\n",
    "    offset = transform_matrix[:2, 2]\n",
    "    img = np.stack([ndimage.interpolation.affine_transform(\n",
    "                    img[:, :, c],\n",
    "                    affine_matrix,\n",
    "                    offset) for c in range(img.shape[2])], axis=2)\n",
    "    img = Image.fromarray(img)\n",
    "    return img\n",
    "\n",
    "\n",
    "def rotate(img, magnitude):\n",
    "    img = np.array(img)\n",
    "    magnitudes = np.linspace(-30, 30, 11)\n",
    "    theta = np.deg2rad(random.uniform(magnitudes[magnitude], magnitudes[magnitude+1]))\n",
    "    transform_matrix = np.array([[np.cos(theta), -np.sin(theta), 0],\n",
    "                                 [np.sin(theta), np.cos(theta), 0],\n",
    "                                 [0, 0, 1]])\n",
    "    transform_matrix = transform_matrix_offset_center(transform_matrix, img.shape[0], img.shape[1])\n",
    "    affine_matrix = transform_matrix[:2, :2]\n",
    "    offset = transform_matrix[:2, 2]\n",
    "    img = np.stack([ndimage.interpolation.affine_transform(\n",
    "                    img[:, :, c],\n",
    "                    affine_matrix,\n",
    "                    offset) for c in range(img.shape[2])], axis=2)\n",
    "    img = Image.fromarray(img)\n",
    "    return img\n",
    "\n",
    "\n",
    "def auto_contrast(img, magnitude):\n",
    "    img = ImageOps.autocontrast(img)\n",
    "    return img\n",
    "\n",
    "\n",
    "def invert(img, magnitude):\n",
    "    img = ImageOps.invert(img)\n",
    "    return img\n",
    "\n",
    "\n",
    "def equalize(img, magnitude):\n",
    "    img = ImageOps.equalize(img)\n",
    "    return img\n",
    "\n",
    "\n",
    "def solarize(img, magnitude):\n",
    "    magnitudes = np.linspace(0, 256, 11)\n",
    "    img = ImageOps.solarize(img, random.uniform(magnitudes[magnitude], magnitudes[magnitude+1]))\n",
    "    return img\n",
    "\n",
    "\n",
    "def posterize(img, magnitude):\n",
    "    magnitudes = np.linspace(4, 8, 11)\n",
    "    img = ImageOps.posterize(img, int(round(random.uniform(magnitudes[magnitude], magnitudes[magnitude+1]))))\n",
    "    return img\n",
    "\n",
    "\n",
    "def contrast(img, magnitude):\n",
    "    magnitudes = np.linspace(0.1, 1.9, 11)\n",
    "    img = ImageEnhance.Contrast(img).enhance(random.uniform(magnitudes[magnitude], magnitudes[magnitude+1]))\n",
    "    return img\n",
    "\n",
    "\n",
    "def color(img, magnitude):\n",
    "    magnitudes = np.linspace(0.1, 1.9, 11)\n",
    "    img = ImageEnhance.Color(img).enhance(random.uniform(magnitudes[magnitude], magnitudes[magnitude+1]))\n",
    "    return img\n",
    "\n",
    "\n",
    "def brightness(img, magnitude):\n",
    "    magnitudes = np.linspace(0.1, 1.9, 11)\n",
    "    img = ImageEnhance.Brightness(img).enhance(random.uniform(magnitudes[magnitude], magnitudes[magnitude+1]))\n",
    "    return img\n",
    "\n",
    "\n",
    "def sharpness(img, magnitude):\n",
    "    magnitudes = np.linspace(0.1, 1.9, 11)\n",
    "    img = ImageEnhance.Sharpness(img).enhance(random.uniform(magnitudes[magnitude], magnitudes[magnitude+1]))\n",
    "    return img\n",
    "\n",
    "\n",
    "def cutout(org_img, magnitude=None):\n",
    "    img = np.array(img)\n",
    "\n",
    "    magnitudes = np.linspace(0, 60/331, 11)\n",
    "\n",
    "    img = np.copy(org_img)\n",
    "    mask_val = img.mean()\n",
    "\n",
    "    if magnitude is None:\n",
    "        mask_size = 16\n",
    "    else:\n",
    "        mask_size = int(round(img.shape[0]*random.uniform(magnitudes[magnitude], magnitudes[magnitude+1])))\n",
    "    top = np.random.randint(0 - mask_size//2, img.shape[0] - mask_size)\n",
    "    left = np.random.randint(0 - mask_size//2, img.shape[1] - mask_size)\n",
    "    bottom = top + mask_size\n",
    "    right = left + mask_size\n",
    "\n",
    "    if top < 0:\n",
    "        top = 0\n",
    "    if left < 0:\n",
    "        left = 0\n",
    "\n",
    "    img[top:bottom, left:right, :].fill(mask_val)\n",
    "\n",
    "    img = Image.fromarray(img)\n",
    "\n",
    "    return img\n",
    "\n",
    "\n",
    "\n",
    "class Cutout(object):\n",
    "    def __init__(self, length=16):\n",
    "        self.length = length\n",
    "\n",
    "    def __call__(self, img):\n",
    "        img = np.array(img)\n",
    "\n",
    "        mask_val = img.mean()\n",
    "\n",
    "        top = np.random.randint(0 - self.length//2, img.shape[0] - self.length)\n",
    "        left = np.random.randint(0 - self.length//2, img.shape[1] - self.length)\n",
    "        bottom = top + self.length\n",
    "        right = left + self.length\n",
    "\n",
    "        top = 0 if top < 0 else top\n",
    "        left = 0 if left < 0 else top\n",
    "\n",
    "        img[top:bottom, left:right, :] = mask_val\n",
    "\n",
    "        img = Image.fromarray(img)\n",
    "\n",
    "        return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-29T04:44:57.140242Z",
     "iopub.status.busy": "2024-11-29T04:44:57.139983Z",
     "iopub.status.idle": "2024-11-29T04:45:03.323837Z",
     "shell.execute_reply": "2024-11-29T04:45:03.323030Z",
     "shell.execute_reply.started": "2024-11-29T04:44:57.140218Z"
    }
   },
   "outputs": [],
   "source": [
    "# import for utils.py\n",
    "\n",
    "import os\n",
    "import torch\n",
    "#import pandas as pd\n",
    "from skimage import io, transform\n",
    "import scipy\n",
    "import numpy as np\n",
    "#import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torchvision import transforms, utils\n",
    "import sklearn.metrics\n",
    "from sklearn.metrics import confusion_matrix, auc, roc_curve, f1_score, classification_report\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "import math\n",
    "from PIL import Image\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "import types\n",
    "\n",
    "# Define ISIC Dataset Class\n",
    "class ISICDataset(Dataset):\n",
    "    \"\"\"ISIC dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, mdlParams, indSet):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            mdlParams (dict): Configuration for loading\n",
    "            indSet (string): Indicates train, val, test\n",
    "        \"\"\"\n",
    "        # Mdlparams\n",
    "        self.mdlParams = mdlParams\n",
    "        # Number of classes\n",
    "        self.numClasses = mdlParams['numClasses']\n",
    "        # Model input size\n",
    "        self.input_size = (np.int32(mdlParams['input_size'][0]),np.int32(mdlParams['input_size'][1]))      \n",
    "        # Whether or not to use ordered cropping \n",
    "        self.orderedCrop = mdlParams['orderedCrop']   \n",
    "        # Number of crops for multi crop eval\n",
    "        self.multiCropEval = mdlParams['multiCropEval']   \n",
    "        # Whether during training same-sized crops should be used\n",
    "        self.same_sized_crop = mdlParams['same_sized_crops']    \n",
    "        # Only downsample\n",
    "        self.only_downsmaple = mdlParams.get('only_downsmaple',False)   \n",
    "        # Potential class balancing option \n",
    "        self.balancing = mdlParams['balance_classes']\n",
    "        # Whether data should be preloaded\n",
    "        self.preload = mdlParams['preload']\n",
    "        # Potentially subtract a mean\n",
    "        self.subtract_set_mean = mdlParams['subtract_set_mean']\n",
    "        # Potential switch for evaluation on the training set\n",
    "        self.train_eval_state = mdlParams['trainSetState']   \n",
    "        # Potential setMean to deduce from channels\n",
    "        self.setMean = mdlParams['setMean'].astype(np.float32)\n",
    "        # Current indSet = 'trainInd'/'valInd'/'testInd'\n",
    "        self.indices = mdlParams[indSet]  \n",
    "        self.indSet = indSet\n",
    "        # feature scaling for meta\n",
    "        if mdlParams.get('meta_features',None) is not None and mdlParams['scale_features']:\n",
    "            self.feature_scaler = mdlParams['feature_scaler_meta']\n",
    "        if self.balancing == 3 and indSet == 'trainInd':\n",
    "            # Sample classes equally for each batch\n",
    "            # First, split set by classes\n",
    "            not_one_hot = np.argmax(mdlParams['labels_array'],1)\n",
    "            self.class_indices = []\n",
    "            for i in range(mdlParams['numClasses']):\n",
    "                self.class_indices.append(np.where(not_one_hot==i)[0])\n",
    "                # Kick out non-trainind indices\n",
    "                self.class_indices[i] = np.setdiff1d(self.class_indices[i],mdlParams['valInd'])\n",
    "                # And test indices\n",
    "                if 'testInd' in mdlParams:\n",
    "                    self.class_indices[i] = np.setdiff1d(self.class_indices[i],mdlParams['testInd'])\n",
    "            # Now sample indices equally for each batch by repeating all of them to have the same amount as the max number\n",
    "            indices = []\n",
    "            max_num = np.max([len(x) for x in self.class_indices])\n",
    "            # Go thourgh all classes\n",
    "            for i in range(mdlParams['numClasses']):\n",
    "                count = 0\n",
    "                class_count = 0\n",
    "                max_num_curr_class = len(self.class_indices[i])\n",
    "                # Add examples until we reach the maximum\n",
    "                while(count < max_num):\n",
    "                    # Start at the beginning, if we are through all available examples\n",
    "                    if class_count == max_num_curr_class:\n",
    "                        class_count = 0\n",
    "                    indices.append(self.class_indices[i][class_count])\n",
    "                    count += 1\n",
    "                    class_count += 1\n",
    "            print(\"Largest class\",max_num,\"Indices len\",len(indices))\n",
    "            print(\"Intersect val\",np.intersect1d(indices,mdlParams['valInd']),\"Intersect Testind\",np.intersect1d(indices,mdlParams['testInd']))\n",
    "            # Set labels/inputs\n",
    "            self.labels = mdlParams['labels_array'][indices,:]\n",
    "            self.im_paths = np.array(mdlParams['im_paths'])[indices].tolist()     \n",
    "            # Normal train proc\n",
    "            if self.same_sized_crop:\n",
    "                cropping = transforms.RandomCrop(self.input_size)\n",
    "            elif self.only_downsmaple:\n",
    "                cropping = transforms.Resize(self.input_size)\n",
    "            else:\n",
    "                cropping = transforms.RandomResizedCrop(self.input_size[0])\n",
    "            # All transforms\n",
    "            self.composed = transforms.Compose([\n",
    "                    cropping,\n",
    "                    transforms.RandomHorizontalFlip(),\n",
    "                    transforms.RandomVerticalFlip(),\n",
    "                    transforms.ColorJitter(brightness=32. / 255.,saturation=0.5),\n",
    "                    transforms.ToTensor(),\n",
    "                    transforms.Normalize(torch.from_numpy(self.setMean).float(),torch.from_numpy(np.array([1.,1.,1.])).float())\n",
    "                    ])                                \n",
    "        elif self.orderedCrop and (indSet == 'valInd' or self.train_eval_state  == 'eval' or indSet == 'testInd'):\n",
    "            # Also flip on top            \n",
    "            if mdlParams.get('eval_flipping',0) > 1:\n",
    "                # Complete labels array, only for current indSet, repeat for multiordercrop\n",
    "                inds_rep = np.repeat(mdlParams[indSet], mdlParams['multiCropEval']*mdlParams['eval_flipping'])\n",
    "                self.labels = mdlParams['labels_array'][inds_rep,:]\n",
    "                # meta\n",
    "                if mdlParams.get('meta_features',None) is not None:\n",
    "                    self.meta_data = mdlParams['meta_array'][inds_rep,:]    \n",
    "                # Path to images for loading, only for current indSet, repeat for multiordercrop\n",
    "                self.im_paths = np.array(mdlParams['im_paths'])[inds_rep].tolist()\n",
    "                print(\"len im path\",len(self.im_paths))                \n",
    "                if self.mdlParams.get('var_im_size',False):\n",
    "                    self.cropPositions = np.tile(mdlParams['cropPositions'][mdlParams[indSet],:,:],(1,mdlParams['eval_flipping'],1))\n",
    "                    self.cropPositions = np.reshape(self.cropPositions,[mdlParams['multiCropEval']*mdlParams['eval_flipping']*mdlParams[indSet].shape[0],2])\n",
    "                    #self.cropPositions = np.repeat(self.cropPositions, (mdlParams['eval_flipping'],1))\n",
    "                    #print(\"CP examples\",self.cropPositions[:50,:])\n",
    "                else:\n",
    "                    self.cropPositions = np.tile(mdlParams['cropPositions'], (mdlParams['eval_flipping']*mdlParams[indSet].shape[0],1))\n",
    "                # Flip states\n",
    "                if mdlParams['eval_flipping'] == 2:\n",
    "                    self.flipPositions = np.array([0,1])\n",
    "                elif mdlParams['eval_flipping'] == 3:\n",
    "                    self.flipPositions = np.array([0,1,2])\n",
    "                elif mdlParams['eval_flipping'] == 4:\n",
    "                    self.flipPositions = np.array([0,1,2,3])                    \n",
    "                self.flipPositions = np.repeat(self.flipPositions, mdlParams['multiCropEval'])\n",
    "                self.flipPositions = np.tile(self.flipPositions, mdlParams[indSet].shape[0])\n",
    "                print(\"Crop positions shape\",self.cropPositions.shape,\"flip pos shape\",self.flipPositions.shape)\n",
    "                print(\"Flip example\",self.flipPositions[:30])\n",
    "            else:\n",
    "                # Complete labels array, only for current indSet, repeat for multiordercrop\n",
    "                inds_rep = np.repeat(mdlParams[indSet], mdlParams['multiCropEval'])\n",
    "                self.labels = mdlParams['labels_array'][inds_rep,:]\n",
    "                # meta\n",
    "                if mdlParams.get('meta_features',None) is not None:\n",
    "                    self.meta_data = mdlParams['meta_array'][inds_rep,:]                 \n",
    "                # Path to images for loading, only for current indSet, repeat for multiordercrop\n",
    "                self.im_paths = np.array(mdlParams['im_paths'])[inds_rep].tolist()\n",
    "                print(\"len im path\",len(self.im_paths))\n",
    "                # Set up crop positions for every sample                \n",
    "                if self.mdlParams.get('var_im_size',False):\n",
    "                    self.cropPositions = np.reshape(mdlParams['cropPositions'][mdlParams[indSet],:,:],[mdlParams['multiCropEval']*mdlParams[indSet].shape[0],2])\n",
    "                    #print(\"CP examples\",self.cropPositions[:50,:])\n",
    "                else:\n",
    "                    self.cropPositions = np.tile(mdlParams['cropPositions'], (mdlParams[indSet].shape[0],1))\n",
    "                print(\"CP\",self.cropPositions.shape)\n",
    "            #print(\"CP Example\",self.cropPositions[0:len(mdlParams['cropPositions']),:])          \n",
    "            # Set up transforms\n",
    "            self.norm = transforms.Normalize(np.float32(self.mdlParams['setMean']),np.float32(self.mdlParams['setStd']))\n",
    "            self.trans = transforms.ToTensor()\n",
    "        elif indSet == 'valInd' or indSet == 'testInd':\n",
    "            if self.multiCropEval == 0:\n",
    "                if self.only_downsmaple:\n",
    "                    self.cropping = transforms.Resize(self.input_size)\n",
    "                else:\n",
    "                    self.cropping = transforms.Compose([transforms.CenterCrop(np.int32(self.input_size[0]*1.5)),transforms.Resize(self.input_size)])\n",
    "                # Complete labels array, only for current indSet\n",
    "                self.labels = mdlParams['labels_array'][mdlParams[indSet],:]\n",
    "                # meta\n",
    "                if mdlParams.get('meta_features',None) is not None:\n",
    "                    self.meta_data = mdlParams['meta_array'][mdlParams[indSet],:]                 \n",
    "                # Path to images for loading, only for current indSet\n",
    "                self.im_paths = np.array(mdlParams['im_paths'])[mdlParams[indSet]].tolist()                   \n",
    "            else:\n",
    "                # Deterministic processing\n",
    "                if self.mdlParams.get('deterministic_eval',False):\n",
    "                    total_len_per_im = mdlParams['numCropPositions']*len(mdlParams['cropScales'])*mdlParams['cropFlipping']                    \n",
    "                    # Actual transforms are functionally applied at forward pass\n",
    "                    self.cropPositions = np.zeros([total_len_per_im,3])\n",
    "                    ind = 0\n",
    "                    for i in range(mdlParams['numCropPositions']):\n",
    "                        for j in range(len(mdlParams['cropScales'])):\n",
    "                            for k in range(mdlParams['cropFlipping']):\n",
    "                                self.cropPositions[ind,0] = i\n",
    "                                self.cropPositions[ind,1] = mdlParams['cropScales'][j]\n",
    "                                self.cropPositions[ind,2] = k\n",
    "                                ind += 1\n",
    "                    # Complete labels array, only for current indSet, repeat for multiordercrop\n",
    "                    print(\"crops per image\",total_len_per_im)\n",
    "                    self.cropPositions = np.tile(self.cropPositions, (mdlParams[indSet].shape[0],1))\n",
    "                    inds_rep = np.repeat(mdlParams[indSet], total_len_per_im)\n",
    "                    self.labels = mdlParams['labels_array'][inds_rep,:]\n",
    "                    # meta\n",
    "                    if mdlParams.get('meta_features',None) is not None:\n",
    "                        self.meta_data = mdlParams['meta_array'][inds_rep,:]                     \n",
    "                    # Path to images for loading, only for current indSet, repeat for multiordercrop\n",
    "                    self.im_paths = np.array(mdlParams['im_paths'])[inds_rep].tolist()\n",
    "                else:\n",
    "                    self.cropping = transforms.RandomResizedCrop(self.input_size[0],scale=(mdlParams.get('scale_min',0.08),1.0))\n",
    "                    # Complete labels array, only for current indSet, repeat for multiordercrop\n",
    "                    inds_rep = np.repeat(mdlParams[indSet], mdlParams['multiCropEval'])\n",
    "                    self.labels = mdlParams['labels_array'][inds_rep,:]\n",
    "                    # meta\n",
    "                    if mdlParams.get('meta_features',None) is not None:\n",
    "                        self.meta_data = mdlParams['meta_array'][inds_rep,:]                    \n",
    "                    # Path to images for loading, only for current indSet, repeat for multiordercrop\n",
    "                    self.im_paths = np.array(mdlParams['im_paths'])[inds_rep].tolist()\n",
    "            print(len(self.im_paths))  \n",
    "            # Set up transforms\n",
    "            self.norm = transforms.Normalize(np.float32(self.mdlParams['setMean']),np.float32(self.mdlParams['setStd']))\n",
    "            self.trans = transforms.ToTensor()                   \n",
    "        else:\n",
    "            all_transforms = []\n",
    "            # Normal train proc\n",
    "            if self.same_sized_crop:\n",
    "                all_transforms.append(transforms.RandomCrop(self.input_size))\n",
    "            elif self.only_downsmaple:\n",
    "                all_transforms.append(transforms.Resize(self.input_size))\n",
    "            else:\n",
    "                all_transforms.append(transforms.RandomResizedCrop(self.input_size[0],scale=(mdlParams.get('scale_min',0.08),1.0)))\n",
    "            if mdlParams.get('flip_lr_ud',False):\n",
    "                all_transforms.append(transforms.RandomHorizontalFlip())\n",
    "                all_transforms.append(transforms.RandomVerticalFlip())\n",
    "            # Full rot\n",
    "            if mdlParams.get('full_rot',0) > 0:\n",
    "                if mdlParams.get('scale',False):\n",
    "                    all_transforms.append(transforms.RandomChoice([transforms.RandomAffine(mdlParams['full_rot'], scale=mdlParams['scale'], shear=mdlParams.get('shear',0), resample=Image.NEAREST),\n",
    "                                                                transforms.RandomAffine(mdlParams['full_rot'],scale=mdlParams['scale'],shear=mdlParams.get('shear',0), resample=Image.BICUBIC),\n",
    "                                                                transforms.RandomAffine(mdlParams['full_rot'],scale=mdlParams['scale'],shear=mdlParams.get('shear',0), resample=Image.BILINEAR)])) \n",
    "                else:\n",
    "                    all_transforms.append(transforms.RandomChoice([transforms.RandomRotation(mdlParams['full_rot'], resample=Image.NEAREST),\n",
    "                                                                transforms.RandomRotation(mdlParams['full_rot'], resample=Image.BICUBIC),\n",
    "                                                                transforms.RandomRotation(mdlParams['full_rot'], resample=Image.BILINEAR)]))    \n",
    "            # Color distortion\n",
    "            if mdlParams.get('full_color_distort') is not None:\n",
    "                all_transforms.append(transforms.ColorJitter(brightness=mdlParams.get('brightness_aug',32. / 255.),saturation=mdlParams.get('saturation_aug',0.5), contrast = mdlParams.get('contrast_aug',0.5), hue = mdlParams.get('hue_aug',0.2)))\n",
    "            else:\n",
    "                all_transforms.append(transforms.ColorJitter(brightness=32. / 255.,saturation=0.5))   \n",
    "            # Autoaugment\n",
    "            if self.mdlParams.get('autoaugment',False):\n",
    "                all_transforms.append(AutoAugment())             \n",
    "            # Cutout\n",
    "            if self.mdlParams.get('cutout',0) > 0:\n",
    "                all_transforms.append(Cutout_v0(n_holes=1,length=self.mdlParams['cutout']))                             \n",
    "            # Normalize\n",
    "            all_transforms.append(transforms.ToTensor())\n",
    "            all_transforms.append(transforms.Normalize(np.float32(self.mdlParams['setMean']),np.float32(self.mdlParams['setStd'])))            \n",
    "            # All transforms\n",
    "            self.composed = transforms.Compose(all_transforms)                  \n",
    "            # Complete labels array, only for current indSet\n",
    "            self.labels = mdlParams['labels_array'][mdlParams[indSet],:]\n",
    "            # meta\n",
    "            if mdlParams.get('meta_features',None) is not None:\n",
    "                self.meta_data = mdlParams['meta_array'][mdlParams[indSet],:]            \n",
    "            # Path to images for loading, only for current indSet\n",
    "            self.im_paths = np.array(mdlParams['im_paths'])[mdlParams[indSet]].tolist()\n",
    "        # Potentially preload\n",
    "        if self.preload:\n",
    "            self.im_list = []\n",
    "            for i in range(len(self.im_paths)):\n",
    "                self.im_list.append(Image.open(self.im_paths[i]))\n",
    "    def __len__(self):\n",
    "        return self.labels.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Load image\n",
    "        if self.preload:\n",
    "            x = self.im_list[idx]\n",
    "        else:\n",
    "            x = Image.open(self.im_paths[idx])\n",
    "            if self.mdlParams.get('resize_large_ones',0) > 0 and (x.size[0] == self.mdlParams['large_size'] and x.size[1] == self.mdlParams['large_size']):\n",
    "                width = self.mdlParams['resize_large_ones']\n",
    "                height = self.mdlParams['resize_large_ones']\n",
    "                #height = (self.mdlParams['resize_large_ones']/self.mdlParams['large_size'])*x.size[1]\n",
    "                x = x.resize((width,height),Image.BILINEAR)\n",
    "            if self.mdlParams['input_size'][0] >= 224 and self.mdlParams['orderedCrop']:\n",
    "                if x.size[0] < self.mdlParams['input_size'][0]:\n",
    "                    new_height = int(self.mdlParams['input_size'][0]/float(x.size[0]))*x.size[1]\n",
    "                    new_width = self.mdlParams['input_size'][0]\n",
    "                    x = x.resize((new_width,new_height),Image.BILINEAR)\n",
    "                if x.size[1] < self.mdlParams['input_size'][0]:\n",
    "                    new_width = int(self.mdlParams['input_size'][0]/float(x.size[1]))*x.size[0]\n",
    "                    new_height = self.mdlParams['input_size'][0]\n",
    "                    x = x.resize((new_width,new_height),Image.BILINEAR)               \n",
    "        # Get label\n",
    "        y = self.labels[idx,:]\n",
    "        # meta\n",
    "        if self.mdlParams.get('meta_features',None) is not None:\n",
    "            x_meta = self.meta_data[idx,:].copy()         \n",
    "        # Transform data based on whether train or not train. If train, also check if its train train or train inference\n",
    "        if self.orderedCrop and (self.indSet == 'valInd' or self.indSet == 'testInd' or self.train_eval_state == 'eval'):\n",
    "            # Apply ordered cropping to validation or test set\n",
    "            # Get current crop position\n",
    "            x_loc = self.cropPositions[idx,0]\n",
    "            y_loc = self.cropPositions[idx,1]\n",
    "            # scale\n",
    "            if self.mdlParams.get('meta_features',None) is not None and self.mdlParams['scale_features']:\n",
    "                x_meta = np.squeeze(self.feature_scaler.transform(np.expand_dims(x_meta,0)))            \n",
    "            if self.mdlParams.get('trans_norm_first',False):\n",
    "                # First, to pytorch tensor (0.0-1.0)\n",
    "                x = self.trans(x)\n",
    "                # Normalize\n",
    "                x = self.norm(x)   \n",
    "                #print(self.im_paths[idx])\n",
    "                #print(\"Before\",x.size(),\"xloc\",x_loc,\"y_loc\",y_loc)\n",
    "                if self.mdlParams.get('eval_flipping',0) > 1:\n",
    "                    if self.flipPositions[idx] == 1:\n",
    "                        x = torch.flip(x,(1,))\n",
    "                    elif self.flipPositions[idx] == 2:\n",
    "                        x = torch.flip(x,(2,))\n",
    "                    elif self.flipPositions[idx] == 3:\n",
    "                        x = torch.flip(x,(1,2))\n",
    "                #print((x_loc-np.int32(self.input_size[0]/2.)),(x_loc-np.int32(self.input_size[0]/2.))+self.input_size[0],(y_loc-np.int32(self.input_size[1]/2.)),(y_loc-np.int32(self.input_size[1]/2.))+self.input_size[1])                \n",
    "                x = x[:,np.int32(x_loc-(self.input_size[0]/2.)):np.int32(x_loc-(self.input_size[0]/2.))+self.input_size[0],\n",
    "                        np.int32(y_loc-(self.input_size[1]/2.)):np.int32(y_loc-(self.input_size[1]/2.))+self.input_size[1]]                 \n",
    "                #print(\"After\",x.size())           \n",
    "            else:\n",
    "                # Then, apply current crop\n",
    "                #print(\"Before\",x.size(),\"xloc\",x_loc,\"y_loc\",y_loc)\n",
    "                #print((x_loc-np.int32(self.input_size[0]/2.)),(x_loc-np.int32(self.input_size[0]/2.))+self.input_size[0],(y_loc-np.int32(self.input_size[1]/2.)),(y_loc-np.int32(self.input_size[1]/2.))+self.input_size[1])\n",
    "                x = Image.fromarray(np.array(x)[(x_loc-np.int32(self.input_size[0]/2.)):(x_loc-np.int32(self.input_size[0]/2.))+self.input_size[0],\n",
    "                        (y_loc-np.int32(self.input_size[1]/2.)):(y_loc-np.int32(self.input_size[1]/2.))+self.input_size[1],:])\n",
    "                # First, to pytorch tensor (0.0-1.0)\n",
    "                x = self.trans(x)\n",
    "                # Normalize\n",
    "                x = self.norm(x)            \n",
    "            #print(\"After\",x.size())\n",
    "        elif self.indSet == 'valInd' or self.indSet == 'testInd':\n",
    "            if self.mdlParams.get('deterministic_eval',False):\n",
    "                crop = self.cropPositions[idx,0]   \n",
    "                scale = self.cropPositions[idx,1]\n",
    "                flipping = self.cropPositions[idx,2]\n",
    "                if flipping == 1:\n",
    "                    # Left flip\n",
    "                    x = transforms.functional.hflip(x)\n",
    "                elif flipping == 2:\n",
    "                    # Right flip\n",
    "                    x = transforms.functional.vflip(x)\n",
    "                elif flipping == 3:\n",
    "                    # Both flip\n",
    "                    x = transforms.functional.hflip(x)\n",
    "                    x = transforms.functional.vflip(x)                    \n",
    "                # Scale\n",
    "                if int(scale*x.size[0]) > self.input_size[0] and int(scale*x.size[1]) > self.input_size[1]:\n",
    "                    x = transforms.functional.resize(x,(int(scale*x.size[0]),int(scale*x.size[1])))\n",
    "                else:\n",
    "                    x = transforms.functional.resize(x,(self.input_size[0],self.input_size[1]))\n",
    "                # Crop\n",
    "                if crop == 0:\n",
    "                    # Center\n",
    "                    x = transforms.functional.center_crop(x,self.input_size[0])\n",
    "                elif crop == 1:\n",
    "                    # upper left\n",
    "                    x = transforms.functional.crop(x, self.mdlParams['offset_crop']*x.size[0], self.mdlParams['offset_crop']*x.size[1], self.input_size[0],self.input_size[1])\n",
    "                elif crop == 2:\n",
    "                    # lower left\n",
    "                    x = transforms.functional.crop(x, self.mdlParams['offset_crop']*x.size[0], (1.0-self.mdlParams['offset_crop'])*x.size[1]-self.input_size[1], self.input_size[0],self.input_size[1]) \n",
    "                elif crop == 3:\n",
    "                    # upper right\n",
    "                    x = transforms.functional.crop(x, (1.0-self.mdlParams['offset_crop'])*x.size[0]-self.input_size[0], self.mdlParams['offset_crop']*x.size[1], self.input_size[0],self.input_size[1])  \n",
    "                elif crop == 4:\n",
    "                    # lower right\n",
    "                    x = transforms.functional.crop(x, (1.0-self.mdlParams['offset_crop'])*x.size[0]-self.input_size[0], (1.0-self.mdlParams['offset_crop'])*x.size[1]-self.input_size[1], self.input_size[0],self.input_size[1])       \n",
    "            else:\n",
    "                x = self.cropping(x)        \n",
    "            # To pytorch tensor (0.0-1.0)\n",
    "            x = self.trans(x)\n",
    "            x = self.norm(x)    \n",
    "            # scale\n",
    "            if self.mdlParams.get('meta_features',None) is not None and self.mdlParams['scale_features']:\n",
    "                x_meta = np.squeeze(self.feature_scaler.transform(np.expand_dims(x_meta,0)))                          \n",
    "        else:\n",
    "            # Apply\n",
    "            x = self.composed(x)\n",
    "            # meta augment\n",
    "            if self.mdlParams.get('meta_features',None) is not None:\n",
    "                if self.mdlParams['drop_augment'] > 0:\n",
    "                    # randomly deactivate a feature\n",
    "                    # age\n",
    "                    if torch.rand(1) < self.mdlParams['drop_augment']:\n",
    "                        if 'age_oh' in self.mdlParams['meta_features']:\n",
    "                            x_meta[0:self.mdlParams['meta_feature_sizes'][0]] = np.zeros([self.mdlParams['meta_feature_sizes'][0]])\n",
    "                        else:\n",
    "                            x_meta[0] = -5\n",
    "                    if torch.rand(1) < self.mdlParams['drop_augment']:\n",
    "                        if 'loc_oh' in self.mdlParams['meta_features']:   \n",
    "                            x_meta[self.mdlParams['meta_feature_sizes'][0]:self.mdlParams['meta_feature_sizes'][0]+self.mdlParams['meta_feature_sizes'][1]] = np.zeros([self.mdlParams['meta_feature_sizes'][1]])\n",
    "                    if torch.rand(1) < self.mdlParams['drop_augment']:\n",
    "                        if 'sex_oh' in self.mdlParams['meta_features']:   \n",
    "                            x_meta[self.mdlParams['meta_feature_sizes'][0]+self.mdlParams['meta_feature_sizes'][1]:self.mdlParams['meta_feature_sizes'][0]+self.mdlParams['meta_feature_sizes'][1]+self.mdlParams['meta_feature_sizes'][2]] = np.zeros([self.mdlParams['meta_feature_sizes'][2]]) \n",
    "                # scale\n",
    "                if self.mdlParams['scale_features']:\n",
    "                    x_meta = np.squeeze(self.feature_scaler.transform(np.expand_dims(x_meta,0)))                         \n",
    "        # Transform y\n",
    "        y = np.argmax(y)\n",
    "        y = np.int64(y)\n",
    "        if self.mdlParams.get('meta_features',None) is not None:\n",
    "            x_meta = np.float32(x_meta) \n",
    "        if self.mdlParams.get('eval_flipping',0) > 1:\n",
    "            return x, y, idx, self.flipPositions[idx]\n",
    "        else:\n",
    "            if self.mdlParams.get('meta_features',None) is not None:\n",
    "                return (x, x_meta), y, idx\n",
    "            else:\n",
    "                return x, y, idx\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class Cutout_v0(object):\n",
    "    \"\"\"Randomly mask out one or more patches from an image.\n",
    "    Args:\n",
    "        n_holes (int): Number of patches to cut out of each image.\n",
    "        length (int): The length (in pixels) of each square patch.\n",
    "    \"\"\"\n",
    "    def __init__(self, n_holes, length):\n",
    "        self.n_holes = n_holes\n",
    "        self.length = length\n",
    "\n",
    "    def __call__(self, img):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            img (Tensor): Tensor image of size (C, H, W).\n",
    "        Returns:\n",
    "            Tensor: Image with n_holes of dimension length x length cut out of it.\n",
    "        \"\"\"\n",
    "        img = np.array(img)\n",
    "        #print(img.shape)\n",
    "        h = img.shape[0]\n",
    "        w = img.shape[1]\n",
    "\n",
    "        mask = np.ones((h, w), np.uint8)\n",
    "\n",
    "        for n in range(self.n_holes):\n",
    "            y = np.random.randint(h)\n",
    "            x = np.random.randint(w)\n",
    "\n",
    "            y1 = np.clip(y - self.length // 2, 0, h)\n",
    "            y2 = np.clip(y + self.length // 2, 0, h)\n",
    "            x1 = np.clip(x - self.length // 2, 0, w)\n",
    "            x2 = np.clip(x + self.length // 2, 0, w)\n",
    "\n",
    "            mask[y1: y2, x1: x2] = 0.\n",
    "\n",
    "        #mask = torch.from_numpy(mask)\n",
    "        #mask = mask.expand_as(img)\n",
    "        img = img * np.expand_dims(mask,axis=2)\n",
    "        img = Image.fromarray(img)\n",
    "        return img    \n",
    "\n",
    "# Sampler for balanced sampling\n",
    "class StratifiedSampler(torch.utils.data.sampler.Sampler):\n",
    "    \"\"\"Stratified Sampling\n",
    "    Provides equal representation of target classes in each batch\n",
    "    \"\"\"\n",
    "    def __init__(self, mdlParams):\n",
    "        \"\"\"\n",
    "        Arguments\n",
    "        ---------\n",
    "        class_vector : torch tensor\n",
    "            a vector of class labels\n",
    "        batch_size : integer\n",
    "            batch_size\n",
    "        \"\"\"\n",
    "        self.dataset_len = len(mdlParams['trainInd'])\n",
    "        self.numClasses = mdlParams['numClasses']\n",
    "        self.trainInd = mdlParams['trainInd']\n",
    "        # Sample classes equally for each batch\n",
    "        # First, split set by classes\n",
    "        not_one_hot = np.argmax(mdlParams['labels_array'][mdlParams['trainInd'],:],1)\n",
    "        self.class_indices = []\n",
    "        for i in range(mdlParams['numClasses']):\n",
    "            self.class_indices.append(np.where(not_one_hot==i)[0])\n",
    "        self.current_class_ind = 0\n",
    "        self.current_in_class_ind = np.zeros([mdlParams['numClasses']],dtype=int)\n",
    "\n",
    "    def gen_sample_array(self):\n",
    "        # Shuffle all classes first\n",
    "        for i in range(self.numClasses):\n",
    "            np.random.shuffle(self.class_indices[i])\n",
    "        # Construct indset\n",
    "        indices = np.zeros([self.dataset_len],dtype=np.int32)\n",
    "        ind = 0\n",
    "        while(ind < self.dataset_len):\n",
    "            indices[ind] = self.class_indices[self.current_class_ind][self.current_in_class_ind[self.current_class_ind]]\n",
    "            # Take care of in-class index\n",
    "            if self.current_in_class_ind[self.current_class_ind] == len(self.class_indices[self.current_class_ind])-1:\n",
    "                self.current_in_class_ind[self.current_class_ind] = 0\n",
    "                # Shuffle\n",
    "                np.random.shuffle(self.class_indices[self.current_class_ind])\n",
    "            else:\n",
    "                self.current_in_class_ind[self.current_class_ind] += 1\n",
    "            # Take care of overall class ind\n",
    "            if self.current_class_ind == self.numClasses-1:\n",
    "                self.current_class_ind = 0\n",
    "            else:\n",
    "                self.current_class_ind += 1\n",
    "            ind += 1\n",
    "        return indices\n",
    "\n",
    "    def __iter__(self):\n",
    "        return iter(self.gen_sample_array())\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.dataset_len \n",
    "\n",
    "class FocalLoss(nn.Module):\n",
    "\n",
    "    def __init__(self, gamma=2.0, alpha=None, size_average=True):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.gamma = gamma\n",
    "        self.alpha = alpha\n",
    "        if isinstance(alpha, (float, int)): self.alpha = torch.Tensor([alpha, 1 - alpha])\n",
    "        if isinstance(alpha, list): self.alpha = torch.Tensor(alpha)\n",
    "        self.size_average = size_average\n",
    "\n",
    "    def forward(self, input, target):\n",
    "        if input.dim()>2:\n",
    "            input = input.view(input.size(0), input.size(1), -1)  # N,C,H,W => N,C,H*W\n",
    "            input = input.transpose(1, 2)                         # N,C,H*W => N,H*W,C\n",
    "            input = input.contiguous().view(-1, input.size(2))    # N,H*W,C => N*H*W,C\n",
    "        target = target.view(-1, 1)\n",
    "\n",
    "        logpt = F.log_softmax(input, dim=1)\n",
    "        #print(\"before gather\",logpt)\n",
    "        #print(\"target\",target)\n",
    "        logpt = logpt.gather(1,target)\n",
    "        #print(\"after gather\",logpt)\n",
    "        logpt = logpt.view(-1)\n",
    "        pt = logpt.exp()\n",
    "\n",
    "        if self.alpha is not None:\n",
    "            if self.alpha.type() != input.data.type():\n",
    "                self.alpha = self.alpha.type_as(input.data)\n",
    "            at = self.alpha.gather(0, target.data.view(-1))\n",
    "            #print(\"alpha\",self.alpha)\n",
    "            #print(\"gathered\",at)\n",
    "            logpt = logpt * at\n",
    "\n",
    "        loss = -1 * (1 - pt)**self.gamma * logpt\n",
    "        if self.size_average: return loss.mean()\n",
    "        else: return loss.sum()\n",
    "\n",
    "def getErrClassification_mgpu(mdlParams, indices, modelVars, exclude_class=None):\n",
    "    \"\"\"Helper function to return the error of a set\n",
    "    Args:\n",
    "      mdlParams: dictionary, configuration file\n",
    "      indices: string, either \"trainInd\", \"valInd\" or \"testInd\"\n",
    "    Returns:\n",
    "      loss: float, avg loss\n",
    "      acc: float, accuracy\n",
    "      sensitivity: float, sensitivity\n",
    "      spec: float, specificity\n",
    "      conf: float matrix, confusion matrix\n",
    "    \"\"\"\n",
    "    # Set up sizes\n",
    "    if indices == 'trainInd':\n",
    "        numBatches = int(math.floor(len(mdlParams[indices])/mdlParams['batchSize']/len(mdlParams['numGPUs'])))\n",
    "    else:\n",
    "        numBatches = int(math.ceil(len(mdlParams[indices])/mdlParams['batchSize']/len(mdlParams['numGPUs'])))\n",
    "    # Consider multi-crop case\n",
    "    if mdlParams.get('eval_flipping',0) > 1 and mdlParams.get('multiCropEval',0) > 0:\n",
    "        loss_all = np.zeros([numBatches])\n",
    "        predictions = np.zeros([len(mdlParams[indices]),mdlParams['numClasses']])\n",
    "        targets = np.zeros([len(mdlParams[indices]),mdlParams['numClasses']])        \n",
    "        loss_mc = np.zeros([len(mdlParams[indices])*mdlParams['eval_flipping']])\n",
    "        predictions_mc = np.zeros([len(mdlParams[indices]),mdlParams['numClasses'],mdlParams['multiCropEval'],mdlParams['eval_flipping']])\n",
    "        targets_mc = np.zeros([len(mdlParams[indices]),mdlParams['numClasses'],mdlParams['multiCropEval'],mdlParams['eval_flipping']])  \n",
    "        # Very suboptimal method\n",
    "        ind = -1\n",
    "        for i, (inputs, labels, inds, flip_ind) in enumerate(modelVars['dataloader_'+indices]):\n",
    "            if flip_ind[0] != np.mean(np.array(flip_ind)):\n",
    "                print(\"Problem with flipping\",flip_ind)\n",
    "            if flip_ind[0] == 0:\n",
    "                ind += 1\n",
    "            # Get data\n",
    "            if mdlParams.get('meta_features',None) is not None: \n",
    "                inputs[0] = inputs[0].cuda()\n",
    "                inputs[1] = inputs[1].cuda()\n",
    "            else:            \n",
    "                inputs = inputs.to(modelVars['device'])\n",
    "            labels = labels.to(modelVars['device'])       \n",
    "            # Not sure if thats necessary\n",
    "            modelVars['optimizer'].zero_grad()    \n",
    "            with torch.set_grad_enabled(False):\n",
    "                # Get outputs\n",
    "                if mdlParams.get('aux_classifier',False):\n",
    "                    outputs, outputs_aux = modelVars['model'](inputs)\n",
    "                    if mdlParams['eval_aux_classifier']:\n",
    "                        outputs = outputs_aux\n",
    "                else:\n",
    "                    outputs = modelVars['model'](inputs)\n",
    "                preds = modelVars['softmax'](outputs)      \n",
    "                # Loss\n",
    "                loss = modelVars['criterion'](outputs, labels)           \n",
    "            # Write into proper arrays\n",
    "            loss_mc[ind] = np.mean(loss.cpu().numpy())\n",
    "            predictions_mc[ind,:,:,flip_ind[0]] = np.transpose(preds.cpu().numpy())\n",
    "            tar_not_one_hot = labels.data.cpu().numpy()\n",
    "            tar = np.zeros((tar_not_one_hot.shape[0], mdlParams['numClasses']))\n",
    "            tar[np.arange(tar_not_one_hot.shape[0]),tar_not_one_hot] = 1\n",
    "            targets_mc[ind,:,:,flip_ind[0]] = np.transpose(tar)\n",
    "        # Targets stay the same\n",
    "        targets = targets_mc[:,:,0,0]\n",
    "        # reshape preds\n",
    "        predictions_mc = np.reshape(predictions_mc,[predictions_mc.shape[0],predictions_mc.shape[1],mdlParams['multiCropEval']*mdlParams['eval_flipping']])\n",
    "        if mdlParams['voting_scheme'] == 'vote':\n",
    "            # Vote for correct prediction\n",
    "            print(\"Pred Shape\",predictions_mc.shape)\n",
    "            predictions_mc = np.argmax(predictions_mc,1)    \n",
    "            print(\"Pred Shape\",predictions_mc.shape) \n",
    "            for j in range(predictions_mc.shape[0]):\n",
    "                predictions[j,:] = np.bincount(predictions_mc[j,:],minlength=mdlParams['numClasses'])   \n",
    "            print(\"Pred Shape\",predictions.shape) \n",
    "        elif mdlParams['voting_scheme'] == 'average':\n",
    "            predictions = np.mean(predictions_mc,2)        \n",
    "    elif mdlParams.get('multiCropEval',0) > 0:\n",
    "        loss_all = np.zeros([numBatches])\n",
    "        predictions = np.zeros([len(mdlParams[indices]),mdlParams['numClasses']])\n",
    "        targets = np.zeros([len(mdlParams[indices]),mdlParams['numClasses']])        \n",
    "        loss_mc = np.zeros([len(mdlParams[indices])])\n",
    "        predictions_mc = np.zeros([len(mdlParams[indices]),mdlParams['numClasses'],mdlParams['multiCropEval']])\n",
    "        targets_mc = np.zeros([len(mdlParams[indices]),mdlParams['numClasses'],mdlParams['multiCropEval']])   \n",
    "        for i, (inputs, labels, inds) in enumerate(modelVars['dataloader_'+indices]):\n",
    "            # Get data\n",
    "            if mdlParams.get('meta_features',None) is not None: \n",
    "                inputs[0] = inputs[0].cuda()\n",
    "                inputs[1] = inputs[1].cuda()\n",
    "            else:            \n",
    "                inputs = inputs.to(modelVars['device'])\n",
    "            labels = labels.to(modelVars['device'])       \n",
    "            # Not sure if thats necessary\n",
    "            modelVars['optimizer'].zero_grad()    \n",
    "            with torch.set_grad_enabled(False):\n",
    "                # Get outputs\n",
    "                if mdlParams.get('aux_classifier',False):\n",
    "                    outputs, outputs_aux = modelVars['model'](inputs)\n",
    "                    if mdlParams['eval_aux_classifier']:\n",
    "                        outputs = outputs_aux\n",
    "                else:\n",
    "                    outputs = modelVars['model'](inputs)\n",
    "                preds = modelVars['softmax'](outputs)      \n",
    "                # Loss\n",
    "                loss = modelVars['criterion'](outputs, labels)           \n",
    "            # Write into proper arrays\n",
    "            loss_mc[i] = np.mean(loss.cpu().numpy())\n",
    "            predictions_mc[i,:,:] = np.transpose(preds.cpu().numpy())\n",
    "            tar_not_one_hot = labels.data.cpu().numpy()\n",
    "            tar = np.zeros((tar_not_one_hot.shape[0], mdlParams['numClasses']))\n",
    "            tar[np.arange(tar_not_one_hot.shape[0]),tar_not_one_hot] = 1\n",
    "            targets_mc[i,:,:] = np.transpose(tar)\n",
    "        # Targets stay the same\n",
    "        targets = targets_mc[:,:,0]\n",
    "        if mdlParams['voting_scheme'] == 'vote':\n",
    "            # Vote for correct prediction\n",
    "            print(\"Pred Shape\",predictions_mc.shape)\n",
    "            predictions_mc = np.argmax(predictions_mc,1)    \n",
    "            print(\"Pred Shape\",predictions_mc.shape) \n",
    "            for j in range(predictions_mc.shape[0]):\n",
    "                predictions[j,:] = np.bincount(predictions_mc[j,:],minlength=mdlParams['numClasses'])   \n",
    "            print(\"Pred Shape\",predictions.shape) \n",
    "        elif mdlParams['voting_scheme'] == 'average':\n",
    "            predictions = np.mean(predictions_mc,2)\n",
    "    else:    \n",
    "        if mdlParams.get('model_type_cnn') is not None and mdlParams['numRandValSeq'] > 0:\n",
    "            loss_all = np.zeros([numBatches])\n",
    "            predictions = np.zeros([len(mdlParams[indices]),mdlParams['numClasses']])\n",
    "            targets = np.zeros([len(mdlParams[indices]),mdlParams['numClasses']])        \n",
    "            loss_mc = np.zeros([len(mdlParams[indices])])\n",
    "            predictions_mc = np.zeros([len(mdlParams[indices]),mdlParams['numClasses'],mdlParams['numRandValSeq']])\n",
    "            targets_mc = np.zeros([len(mdlParams[indices]),mdlParams['numClasses'],mdlParams['numRandValSeq']])   \n",
    "            for i, (inputs, labels, inds) in enumerate(modelVars['dataloader_'+indices]):\n",
    "                # Get data\n",
    "                if mdlParams.get('meta_features',None) is not None: \n",
    "                    inputs[0] = inputs[0].cuda()\n",
    "                    inputs[1] = inputs[1].cuda()\n",
    "                else:            \n",
    "                    inputs = inputs.to(modelVars['device'])\n",
    "                labels = labels.to(modelVars['device'])       \n",
    "                # Not sure if thats necessary\n",
    "                modelVars['optimizer'].zero_grad()    \n",
    "                with torch.set_grad_enabled(False):\n",
    "                    # Get outputs\n",
    "                    if mdlParams.get('aux_classifier',False):\n",
    "                        outputs, outputs_aux = modelVars['model'](inputs)\n",
    "                        if mdlParams['eval_aux_classifier']:\n",
    "                            outputs = outputs_aux\n",
    "                    else:\n",
    "                        outputs = modelVars['model'](inputs)\n",
    "                    preds = modelVars['softmax'](outputs)      \n",
    "                    # Loss\n",
    "                    loss = modelVars['criterion'](outputs, labels)           \n",
    "                # Write into proper arrays\n",
    "                loss_mc[i] = np.mean(loss.cpu().numpy())\n",
    "                predictions_mc[i,:,:] = np.transpose(preds)\n",
    "                tar_not_one_hot = labels.data.cpu().numpy()\n",
    "                tar = np.zeros((tar_not_one_hot.shape[0], mdlParams['numClasses']))\n",
    "                tar[np.arange(tar_not_one_hot.shape[0]),tar_not_one_hot] = 1\n",
    "                targets_mc[i,:,:] = np.transpose(tar)\n",
    "            # Targets stay the same\n",
    "            targets = targets_mc[:,:,0]\n",
    "            if mdlParams['voting_scheme'] == 'vote':\n",
    "                # Vote for correct prediction\n",
    "                print(\"Pred Shape\",predictions_mc.shape)\n",
    "                predictions_mc = np.argmax(predictions_mc,1)    \n",
    "                print(\"Pred Shape\",predictions_mc.shape) \n",
    "                for j in range(predictions_mc.shape[0]):\n",
    "                    predictions[j,:] = np.bincount(predictions_mc[j,:],minlength=mdlParams['numClasses'])   \n",
    "                print(\"Pred Shape\",predictions.shape) \n",
    "            elif mdlParams['voting_scheme'] == 'average':\n",
    "                predictions = np.mean(predictions_mc,2)\n",
    "        else:\n",
    "            for i, (inputs, labels, indices) in enumerate(modelVars['dataloader_'+indices]):\n",
    "                # Get data\n",
    "                if mdlParams.get('meta_features',None) is not None: \n",
    "                    inputs[0] = inputs[0].cuda()\n",
    "                    inputs[1] = inputs[1].cuda()\n",
    "                else:            \n",
    "                    inputs = inputs.to(modelVars['device'])\n",
    "                labels = labels.to(modelVars['device'])       \n",
    "                # Not sure if thats necessary\n",
    "                modelVars['optimizer'].zero_grad()    \n",
    "                with torch.set_grad_enabled(False):\n",
    "                    # Get outputs\n",
    "                    if mdlParams.get('aux_classifier',False):\n",
    "                        outputs, outputs_aux = modelVars['model'](inputs)\n",
    "                        if mdlParams['eval_aux_classifier']:\n",
    "                            outputs = outputs_aux\n",
    "                    else:\n",
    "                        outputs = modelVars['model'](inputs)\n",
    "                    #print(\"in\",inputs.shape,\"out\",outputs.shape)\n",
    "                    preds = modelVars['softmax'](outputs)      \n",
    "                    # Loss\n",
    "                    loss = modelVars['criterion'](outputs, labels)     \n",
    "                # Write into proper arrays                \n",
    "                if i==0:\n",
    "                    loss_all = np.array([loss.cpu().numpy()])\n",
    "                    predictions = preds.cpu().numpy()\n",
    "                    tar_not_one_hot = labels.data.cpu().numpy()\n",
    "                    tar = np.zeros((tar_not_one_hot.shape[0], mdlParams['numClasses']))\n",
    "                    tar[np.arange(tar_not_one_hot.shape[0]),tar_not_one_hot] = 1   \n",
    "                    targets = tar    \n",
    "                    #print(\"Loss\",loss_all)         \n",
    "                else:                 \n",
    "                    loss_all = np.concatenate((loss_all,np.array([loss.cpu().numpy()])),0)\n",
    "                    predictions = np.concatenate((predictions,preds.cpu().numpy()),0)\n",
    "                    tar_not_one_hot = labels.data.cpu().numpy()\n",
    "                    tar = np.zeros((tar_not_one_hot.shape[0], mdlParams['numClasses']))\n",
    "                    tar[np.arange(tar_not_one_hot.shape[0]),tar_not_one_hot] = 1                   \n",
    "                    targets = np.concatenate((targets,tar),0)\n",
    "                    #allInds[(i*len(mdlParams['numGPUs'])+k)*bSize:(i*len(mdlParams['numGPUs'])+k+1)*bSize] = res_tuple[3][k]\n",
    "            predictions_mc = predictions\n",
    "    #print(\"Check Inds\",np.setdiff1d(allInds,mdlParams[indices]))\n",
    "    # Calculate metrics\n",
    "    if exclude_class is not None:\n",
    "        predictions = np.concatenate((predictions[:,:exclude_class],predictions[:,exclude_class+1:]),1)\n",
    "        targets = np.concatenate((targets[:,:exclude_class],targets[:,exclude_class+1:]),1)    \n",
    "        num_classes = mdlParams['numClasses']-1\n",
    "    elif mdlParams['numClasses'] == 9 and mdlParams.get('no_c9_eval',False):\n",
    "        predictions = predictions[:,:mdlParams['numClasses']-1]\n",
    "        targets = targets[:,:mdlParams['numClasses']-1]\n",
    "        num_classes = mdlParams['numClasses']-1\n",
    "    else:\n",
    "        num_classes = mdlParams['numClasses']\n",
    "    # Accuarcy\n",
    "    acc = np.mean(np.equal(np.argmax(predictions,1),np.argmax(targets,1)))\n",
    "    # Confusion matrix\n",
    "    conf = confusion_matrix(np.argmax(targets,1),np.argmax(predictions,1))\n",
    "    if conf.shape[0] < num_classes:\n",
    "        conf = np.ones([num_classes,num_classes])\n",
    "    # Class weighted accuracy\n",
    "    wacc = conf.diagonal()/conf.sum(axis=1)    \n",
    "    # Sensitivity / Specificity\n",
    "    sensitivity = np.zeros([num_classes])\n",
    "    specificity = np.zeros([num_classes])\n",
    "    if num_classes > 2:\n",
    "        for k in range(num_classes):\n",
    "                sensitivity[k] = conf[k,k]/(np.sum(conf[k,:]))\n",
    "                true_negative = np.delete(conf,[k],0)\n",
    "                true_negative = np.delete(true_negative,[k],1)\n",
    "                true_negative = np.sum(true_negative)\n",
    "                false_positive = np.delete(conf,[k],0)\n",
    "                false_positive = np.sum(false_positive[:,k])\n",
    "                specificity[k] = true_negative/(true_negative+false_positive)\n",
    "                # F1 score\n",
    "                f1 = f1_score(np.argmax(predictions,1),np.argmax(targets,1),average='weighted')                \n",
    "    else:\n",
    "        tn, fp, fn, tp = confusion_matrix(np.argmax(targets,1),np.argmax(predictions,1)).ravel()\n",
    "        sensitivity = tp/(tp+fn)\n",
    "        specificity = tn/(tn+fp)\n",
    "        # F1 score\n",
    "        f1 = f1_score(np.argmax(predictions,1),np.argmax(targets,1))\n",
    "    # AUC\n",
    "    fpr = {}\n",
    "    tpr = {}\n",
    "    roc_auc = np.zeros([num_classes])\n",
    "    if num_classes > 9:\n",
    "        print(predictions)\n",
    "    for i in range(num_classes):\n",
    "        fpr[i], tpr[i], _ = roc_curve(targets[:, i], predictions[:, i])\n",
    "        roc_auc[i] = sklearn.metrics.auc(fpr[i], tpr[i])\n",
    "    return np.mean(loss_all), acc, sensitivity, specificity, conf, f1, roc_auc, wacc, predictions, targets, predictions_mc \n",
    "\n",
    "\n",
    "def modify_densenet_avg_pool(model):\n",
    "    def logits(self, features):\n",
    "        x = F.relu(features, inplace=True)\n",
    "        x = torch.mean(torch.mean(x,2), 2)\n",
    "        #x = x.view(x.size(0), -1)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "    def forward(self, input):\n",
    "        x = self.features(input)\n",
    "        x = self.logits(x)\n",
    "        return x\n",
    "\n",
    "    # Modify methods\n",
    "    model.logits = types.MethodType(logits, model)\n",
    "    model.forward = types.MethodType(forward, model)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-29T04:45:03.325874Z",
     "iopub.status.busy": "2024-11-29T04:45:03.325183Z",
     "iopub.status.idle": "2024-11-29T04:45:11.382413Z",
     "shell.execute_reply": "2024-11-29T04:45:11.381164Z",
     "shell.execute_reply.started": "2024-11-29T04:45:03.325833Z"
    }
   },
   "outputs": [],
   "source": [
    "!pip install imagesize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-29T04:45:11.384462Z",
     "iopub.status.busy": "2024-11-29T04:45:11.384196Z",
     "iopub.status.idle": "2024-11-29T04:55:51.170307Z",
     "shell.execute_reply": "2024-11-29T04:55:51.169546Z",
     "shell.execute_reply.started": "2024-11-29T04:45:11.384436Z"
    }
   },
   "outputs": [],
   "source": [
    "# import config file\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import h5py\n",
    "import re\n",
    "import csv\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "import scipy\n",
    "import pickle\n",
    "import imagesize\n",
    "import pdb\n",
    "\n",
    "mdlParams = {}\n",
    "# Save summaries and model here\n",
    "mdlParams['saveDir'] = '/kaggle/working/'\n",
    "# Data is loaded from here\n",
    "# mdlParams['dataDir'] = mdlParams_['pathBase']+\"/\"\n",
    "\n",
    "### Model Selection ###\n",
    "mdlParams['model_type'] = 'isic2019-efficientnet-b0'\n",
    "mdlParams['dataset_names'] = ['official']#,'sevenpoint_rez3_ll']\n",
    "mdlParams['file_ending'] = '.jpg'\n",
    "mdlParams['exclude_inds'] = False\n",
    "mdlParams['same_sized_crops'] = True\n",
    "mdlParams['multiCropEval'] = 9\n",
    "mdlParams['var_im_size'] = True\n",
    "mdlParams['orderedCrop'] = True\n",
    "mdlParams['voting_scheme'] = 'average'    \n",
    "mdlParams['classification'] = True\n",
    "mdlParams['balance_classes'] = 2\n",
    "mdlParams['extra_fac'] = 1.0\n",
    "mdlParams['numClasses'] = 2\n",
    "mdlParams['no_c9_eval'] = True\n",
    "mdlParams['numOut'] = mdlParams['numClasses']\n",
    "mdlParams['numCV'] = 5\n",
    "mdlParams['trans_norm_first'] = True\n",
    "# Scale up for b1-b7\n",
    "mdlParams['input_size'] = [224,224,3]     \n",
    "\n",
    "### Training Parameters ###\n",
    "# Batch size\n",
    "mdlParams['batchSize'] = 20#*len(mdlParams['numGPUs'])\n",
    "# Initial learning rate\n",
    "mdlParams['learning_rate'] = 0.000015#*len(mdlParams['numGPUs'])\n",
    "# Lower learning rate after no improvement over 100 epochs\n",
    "mdlParams['lowerLRAfter'] = 25\n",
    "# If there is no validation set, start lowering the LR after X steps\n",
    "mdlParams['lowerLRat'] = 50\n",
    "# Divide learning rate by this value\n",
    "mdlParams['LRstep'] = 5\n",
    "# Maximum number of training iterations\n",
    "mdlParams['training_steps'] = 60 #250\n",
    "# Display error every X steps\n",
    "mdlParams['display_step'] = 10\n",
    "# Scale?\n",
    "mdlParams['scale_targets'] = False\n",
    "# Peak at test error during training? (generally, dont do this!)\n",
    "mdlParams['peak_at_testerr'] = False\n",
    "# Print trainerr\n",
    "mdlParams['print_trainerr'] = False\n",
    "# Subtract trainset mean?\n",
    "mdlParams['subtract_set_mean'] = False\n",
    "mdlParams['setMean'] = np.array([0.0, 0.0, 0.0])   \n",
    "mdlParams['setStd'] = np.array([1.0, 1.0, 1.0])   \n",
    "\n",
    "# Data AUG\n",
    "#mdlParams['full_color_distort'] = True\n",
    "mdlParams['autoaugment'] = False     \n",
    "mdlParams['flip_lr_ud'] = True\n",
    "mdlParams['full_rot'] = 180\n",
    "mdlParams['scale'] = (0.8,1.2)\n",
    "mdlParams['shear'] = 10\n",
    "mdlParams['cutout'] = 16\n",
    "\n",
    "### Data ###\n",
    "mdlParams['preload'] = False\n",
    "# Labels first\n",
    "# Targets, as dictionary, indexed by im file name\n",
    "mdlParams['labels_dict'] = {}\n",
    " # All sets\n",
    "allSets = ['official']  \n",
    "# Go through all sets\n",
    "for i in range(len(allSets)):\n",
    "    # Check if want to include this dataset\n",
    "    foundSet = False\n",
    "    for j in range(len(mdlParams['dataset_names'])):\n",
    "        if mdlParams['dataset_names'][j] in allSets[i]:\n",
    "            foundSet = True\n",
    "    if not foundSet:\n",
    "        continue                \n",
    "    # Find csv file\n",
    "    file_loc = '/kaggle/input/labels/labels2024.csv'\n",
    "\n",
    "    # Load csv file\n",
    "    with open(file_loc, newline='') as csvfile:\n",
    "        labels_str = csv.reader(csvfile, delimiter=',', quotechar='|')\n",
    "        for row in labels_str:\n",
    "            if 'Malignant' == row[1]:\n",
    "                continue\n",
    "            #if 'ISIC' in row[0] and '_downsampled' in row[0]:\n",
    "            #    print(row[0])\n",
    "            if row[0] + '_downsampled' in mdlParams['labels_dict']:\n",
    "                print(\"removed\",row[0] + '_downsampled')\n",
    "                continue\n",
    "            if mdlParams['numClasses'] == 1:\n",
    "                mdlParams['labels_dict'][row[0]] = np.array([int(float(row[1]))])\n",
    "            if mdlParams['numClasses'] == 2:\n",
    "                mdlParams['labels_dict'][row[0]] = np.array([int(float(row[1])), int(float(row[2]))])\n",
    "            if mdlParams['numClasses'] == 7:\n",
    "                mdlParams['labels_dict'][row[0]] = np.array([int(float(row[1])),int(float(row[2])),int(float(row[3])),int(float(row[4])),int(float(row[5])),int(float(row[6])),int(float(row[7]))])\n",
    "            elif mdlParams['numClasses'] == 8:\n",
    "                if len(row) < 9 or row[8] == '':\n",
    "                    class_8 = 0\n",
    "                else:\n",
    "                    class_8 = int(float(row[8]))\n",
    "                mdlParams['labels_dict'][row[0]] = np.array([int(float(row[1])),int(float(row[2])),int(float(row[3])),int(float(row[4])),int(float(row[5])),int(float(row[6])),int(float(row[7])),class_8])\n",
    "            elif mdlParams['numClasses'] == 9:\n",
    "                if len(row) < 9 or row[8] == '':\n",
    "                    class_8 = 0\n",
    "                else:\n",
    "                    class_8 = int(float(row[8]))  \n",
    "                if len(row) < 10 or row[9] == '':\n",
    "                    class_9 = 0\n",
    "                else:\n",
    "                    class_9 = int(float(row[9]))\n",
    "                mdlParams['labels_dict'][row[0]] = np.array([int(float(row[1])),int(float(row[2])),int(float(row[3])),int(float(row[4])),int(float(row[5])),int(float(row[6])),int(float(row[7])),class_8,class_9])\n",
    "# Save all im paths here\n",
    "mdlParams['im_paths'] = []\n",
    "mdlParams['labels_list'] = []\n",
    "# Define the sets\n",
    "# All sets\n",
    "allSets = ['official']\n",
    "# Ids which name the folders\n",
    "# Make official first dataset\n",
    "for i in range(len(allSets)):\n",
    "    if mdlParams['dataset_names'][0] in allSets[i]:\n",
    "        temp = allSets[i]\n",
    "        allSets.remove(allSets[i])\n",
    "        allSets.insert(0, temp)\n",
    "print(allSets)        \n",
    "# Set of keys, for marking old HAM10000\n",
    "mdlParams['key_list'] = []\n",
    "if mdlParams['exclude_inds']:\n",
    "    with open(mdlParams['saveDir'] + 'indices_exclude.pkl','rb') as f:\n",
    "        indices_exclude = pickle.load(f)          \n",
    "    exclude_list = []   \n",
    "for i in range(len(allSets)):\n",
    "    # All files in that set\n",
    "    files = sorted(glob('/kaggle/input/isic-2024-challenge/train-image/image/*'))\n",
    "    # Check if there is something in there, if not, discard\n",
    "    if len(files) == 0:\n",
    "        continue\n",
    "    # Check if want to include this dataset\n",
    "    foundSet = False\n",
    "    for j in range(len(mdlParams['dataset_names'])):\n",
    "        if mdlParams['dataset_names'][j] in allSets[i]:\n",
    "            foundSet = True\n",
    "    if not foundSet:\n",
    "        continue                    \n",
    "    for j in range(len(files)):\n",
    "        if '.jpg' in files[j] or '.jpeg' in files[j] or '.JPG' in files[j] or '.JPEG' in files[j] or '.png' in files[j] or '.PNG' in files[j]:                \n",
    "            # Add according label, find it first\n",
    "            found_already = False\n",
    "            for key in mdlParams['labels_dict']:\n",
    "                if key + mdlParams['file_ending'] in files[j]:\n",
    "                    if found_already:\n",
    "                        print(\"Found already:\",key,files[j])                     \n",
    "                    mdlParams['key_list'].append(key)\n",
    "                    mdlParams['labels_list'].append(mdlParams['labels_dict'][key])\n",
    "                    found_already = True\n",
    "            if found_already:\n",
    "                mdlParams['im_paths'].append(files[j])     \n",
    "                if mdlParams['exclude_inds']:\n",
    "                    for key in indices_exclude:\n",
    "                        if key in files[j]:\n",
    "                            exclude_list.append(indices_exclude[key])                                       \n",
    "# Convert label list to array\n",
    "mdlParams['labels_array'] = np.array(mdlParams['labels_list'])\n",
    "print(np.mean(mdlParams['labels_array'],axis=0))\n",
    "\n",
    "# Create indices list with HAM10000 only\n",
    "# mdlParams['HAM10000_inds'] = []\n",
    "# HAM_START = 24306\n",
    "# HAM_END = 34320\n",
    "# for j in range(len(mdlParams['key_list'])):\n",
    "#     try:\n",
    "#         curr_id = [int(s) for s in re.findall(r'\\d+',mdlParams['key_list'][j])][-1]\n",
    "#     except:\n",
    "#         continue\n",
    "#     if curr_id >= HAM_START and curr_id <= HAM_END:\n",
    "#         mdlParams['HAM10000_inds'].append(j)\n",
    "# mdlParams['HAM10000_inds'] = np.array(mdlParams['HAM10000_inds'])    \n",
    "# print(\"Len ham\",len(mdlParams['HAM10000_inds']))\n",
    "\n",
    "# Perhaps preload images\n",
    "if mdlParams['preload']:\n",
    "    mdlParams['images_array'] = np.zeros([len(mdlParams['im_paths']),mdlParams['input_size_load'][0],mdlParams['input_size_load'][1],mdlParams['input_size_load'][2]],dtype=np.uint8)\n",
    "    for i in range(len(mdlParams['im_paths'])):\n",
    "        x = scipy.ndimage.imread(mdlParams['im_paths'][i])\n",
    "        #x = x.astype(np.float32)   \n",
    "        # Scale to 0-1 \n",
    "        #min_x = np.min(x)\n",
    "        #max_x = np.max(x)\n",
    "        #x = (x-min_x)/(max_x-min_x)\n",
    "        mdlParams['images_array'][i,:,:,:] = x\n",
    "        if i%1000 == 0:\n",
    "            print(i+1,\"images loaded...\")     \n",
    "if mdlParams['subtract_set_mean']:\n",
    "    mdlParams['images_means'] = np.zeros([len(mdlParams['im_paths']),3])\n",
    "    for i in range(len(mdlParams['im_paths'])):\n",
    "        x = scipy.ndimage.imread(mdlParams['im_paths'][i])\n",
    "        x = x.astype(np.float32)   \n",
    "        # Scale to 0-1 \n",
    "        min_x = np.min(x)\n",
    "        max_x = np.max(x)\n",
    "        x = (x-min_x)/(max_x-min_x)\n",
    "        mdlParams['images_means'][i,:] = np.mean(x,(0,1))\n",
    "        if i%1000 == 0:\n",
    "            print(i+1,\"images processed for mean...\")         \n",
    "\n",
    "### Define Indices ###\n",
    "indices_path = '/kaggle/input/extrafiles/indices_isic2024.pkl';\n",
    "\n",
    "with open(indices_path,'rb') as f:\n",
    "    indices = pickle.load(f)            \n",
    "mdlParams['trainIndCV'] = indices['trainIndCV']\n",
    "mdlParams['valIndCV'] = indices['valIndCV']\n",
    "if mdlParams['exclude_inds']:\n",
    "    exclude_list = np.array(exclude_list)\n",
    "    all_inds = np.arange(len(mdlParams['im_paths']))\n",
    "    exclude_inds = all_inds[exclude_list.astype(bool)]\n",
    "    for i in range(len(mdlParams['trainIndCV'])):\n",
    "        mdlParams['trainIndCV'][i] = np.setdiff1d(mdlParams['trainIndCV'][i],exclude_inds)\n",
    "    for i in range(len(mdlParams['valIndCV'])):\n",
    "        mdlParams['valIndCV'][i] = np.setdiff1d(mdlParams['valIndCV'][i],exclude_inds)     \n",
    "# Consider case with more than one set\n",
    "if len(mdlParams['dataset_names']) > 1:\n",
    "    restInds = np.array(np.arange(25331,mdlParams['labels_array'].shape[0]))\n",
    "    for i in range(mdlParams['numCV']):\n",
    "        mdlParams['trainIndCV'][i] = np.concatenate((mdlParams['trainIndCV'][i],restInds))        \n",
    "print(\"Train\")\n",
    "for i in range(len(mdlParams['trainIndCV'])):\n",
    "    print(mdlParams['trainIndCV'][i].shape)\n",
    "print(\"Val\")\n",
    "for i in range(len(mdlParams['valIndCV'])):\n",
    "    print(mdlParams['valIndCV'][i].shape)    \n",
    "\n",
    "# Use this for ordered multi crops\n",
    "if mdlParams['orderedCrop']:\n",
    "    # Crop positions, always choose multiCropEval to be 4, 9, 16, 25, etc.\n",
    "    mdlParams['cropPositions'] = np.zeros([len(mdlParams['im_paths']),mdlParams['multiCropEval'],2],dtype=np.int64)\n",
    "    #mdlParams['imSizes'] = np.zeros([len(mdlParams['im_paths']),mdlParams['multiCropEval'],2],dtype=np.int64)\n",
    "    for u in range(len(mdlParams['im_paths'])):\n",
    "        height, width = imagesize.get(mdlParams['im_paths'][u])\n",
    "        if width < mdlParams['input_size'][0]:\n",
    "            height = int(mdlParams['input_size'][0]/float(width))*height\n",
    "            width = mdlParams['input_size'][0]\n",
    "        if height < mdlParams['input_size'][0]:\n",
    "            width = int(mdlParams['input_size'][0]/float(height))*width\n",
    "            height = mdlParams['input_size'][0]            \n",
    "        ind = 0\n",
    "        for i in range(np.int32(np.sqrt(mdlParams['multiCropEval']))):\n",
    "            for j in range(np.int32(np.sqrt(mdlParams['multiCropEval']))):\n",
    "                mdlParams['cropPositions'][u,ind,0] = mdlParams['input_size'][0]/2+i*((width-mdlParams['input_size'][1])/(np.sqrt(mdlParams['multiCropEval'])-1))\n",
    "                mdlParams['cropPositions'][u,ind,1] = mdlParams['input_size'][1]/2+j*((height-mdlParams['input_size'][0])/(np.sqrt(mdlParams['multiCropEval'])-1))\n",
    "                #mdlParams['imSizes'][u,ind,0] = curr_im_size[0]\n",
    "\n",
    "                ind += 1\n",
    "    # Sanity checks\n",
    "    #print(\"Positions\",mdlParams['cropPositions'])\n",
    "    # Test image sizes\n",
    "    height = mdlParams['input_size'][0]\n",
    "    width = mdlParams['input_size'][1]\n",
    "    for u in range(len(mdlParams['im_paths'])):\n",
    "        height_test, width_test = imagesize.get(mdlParams['im_paths'][u])\n",
    "        if width_test < mdlParams['input_size'][0]:\n",
    "            height_test = int(mdlParams['input_size'][0]/float(width_test))*height_test\n",
    "            width_test = mdlParams['input_size'][0]\n",
    "        if height_test < mdlParams['input_size'][0]:\n",
    "            width_test = int(mdlParams['input_size'][0]/float(height_test))*width_test\n",
    "            height_test = mdlParams['input_size'][0]                \n",
    "        test_im = np.zeros([width_test,height_test]) \n",
    "        for i in range(mdlParams['multiCropEval']):\n",
    "            im_crop = test_im[np.int32(mdlParams['cropPositions'][u,i,0]-height/2):np.int32(mdlParams['cropPositions'][u,i,0]-height/2)+height,np.int32(mdlParams['cropPositions'][u,i,1]-width/2):np.int32(mdlParams['cropPositions'][u,i,1]-width/2)+width]\n",
    "            if im_crop.shape[0] != mdlParams['input_size'][0]:\n",
    "                print(\"Wrong shape\",im_crop.shape[0],mdlParams['im_paths'][u])    \n",
    "            if im_crop.shape[1] != mdlParams['input_size'][1]:\n",
    "                print(\"Wrong shape\",im_crop.shape[1],mdlParams['im_paths'][u])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-29T04:55:51.173097Z",
     "iopub.status.busy": "2024-11-29T04:55:51.172811Z",
     "iopub.status.idle": "2024-11-29T05:14:05.547381Z",
     "shell.execute_reply": "2024-11-29T05:14:05.544466Z",
     "shell.execute_reply.started": "2024-11-29T04:55:51.173071Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "import torchvision\n",
    "from torchvision import datasets, models as tv_models\n",
    "from torch.utils.data import DataLoader\n",
    "from torchsummary import summary\n",
    "import numpy as np\n",
    "from scipy import io\n",
    "import threading\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "import math\n",
    "import os\n",
    "import sys\n",
    "from glob import glob\n",
    "import re\n",
    "import gc\n",
    "import importlib\n",
    "import time\n",
    "import sklearn.preprocessing\n",
    "from sklearn.utils import class_weight\n",
    "import psutil\n",
    "\n",
    "# Import machine config\n",
    "# pc_cfg = importlib.import_module('pc_cfgs.'+sys.argv[1])\n",
    "# mdlParams.update(pc_cfg.mdlParams)\n",
    "# example. py\n",
    "\n",
    "mdlParams['saveDirBase'] = '/kaggle/working'\n",
    "\n",
    "# Indicate training\n",
    "mdlParams['trainSetState'] = 'train'\n",
    "\n",
    "# Set visible devices\n",
    "if 'gpu' in 'gpu0':\n",
    "    mdlParams['numGPUs']= [[int(s) for s in re.findall(r'\\d+','gpu0')][-1]]\n",
    "    cuda_str = \"\"\n",
    "    for i in range(len(mdlParams['numGPUs'])):\n",
    "        cuda_str = cuda_str + str(mdlParams['numGPUs'][i])\n",
    "        if i is not len(mdlParams['numGPUs'])-1:\n",
    "            cuda_str = cuda_str + \",\"\n",
    "    print(\"Devices to use:\",cuda_str)\n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = cuda_str      \n",
    "\n",
    "# Check if there is a validation set, if not, evaluate train error instead\n",
    "if 'valIndCV' in mdlParams or 'valInd' in mdlParams:\n",
    "    eval_set = 'valInd'\n",
    "    print(\"Evaluating on validation set during training.\")\n",
    "else:\n",
    "    eval_set = 'trainInd'\n",
    "    print(\"No validation set, evaluating on training set during training.\")\n",
    "\n",
    "# Check if there were previous ones that have alreary bin learned\n",
    "prevFile = Path(mdlParams['saveDirBase'] + '/CV.pkl')\n",
    "#print(prevFile)\n",
    "if prevFile.exists():\n",
    "    print(\"Part of CV already done\")\n",
    "    with open(mdlParams['saveDirBase'] + '/CV.pkl', 'rb') as f:\n",
    "        allData = pickle.load(f)\n",
    "else:\n",
    "    allData = {}\n",
    "    allData['f1Best'] = {}\n",
    "    allData['sensBest'] = {}\n",
    "    allData['specBest'] = {}\n",
    "    allData['accBest'] = {}\n",
    "    allData['waccBest'] = {}\n",
    "    allData['aucBest'] = {}\n",
    "    allData['convergeTime'] = {}\n",
    "    allData['bestPred'] = {}\n",
    "    allData['targets'] = {}\n",
    "\n",
    "\n",
    "# Take care of CV\n",
    "if mdlParams.get('cv_subset',None) is not None:\n",
    "    cv_set = mdlParams['cv_subset']\n",
    "else:\n",
    "    cv_set = range(mdlParams['numCV'])\n",
    "for cv in cv_set:  \n",
    "    # Check if this fold was already trained\n",
    "    already_trained = False\n",
    "    if 'valIndCV' in mdlParams:\n",
    "        mdlParams['saveDir'] = '/kaggle/input/output' + '/CVSet' + str(cv)\n",
    "        if os.path.isdir('/kaggle/input/output'):\n",
    "            if os.path.isdir(mdlParams['saveDir']):\n",
    "                all_max_iter = []\n",
    "                for name in os.listdir(mdlParams['saveDir']):\n",
    "                    int_list = [int(s) for s in re.findall(r'\\d+',name)]\n",
    "                    if len(int_list) > 0:\n",
    "                        all_max_iter.append(int_list[-1])\n",
    "                    #if '-' + str(mdlParams['training_steps'])+ '.pt' in name:\n",
    "                    #    print(\"Fold %d already fully trained\"%(cv))\n",
    "                    #    already_trained = True\n",
    "                all_max_iter = np.array(all_max_iter)\n",
    "                if len(all_max_iter) > 0 and np.max(all_max_iter) >= mdlParams['training_steps']:\n",
    "                    print(\"Fold %d already fully trained with %d iterations\"%(cv,np.max(all_max_iter)))\n",
    "                    already_trained = True\n",
    "    if already_trained:\n",
    "        continue        \n",
    "    print(\"CV set\",cv)\n",
    "    # Reset model graph \n",
    "    # importlib.reload(models)\n",
    "    # importlib.reload(torchvision)\n",
    "    # Collect model variables\n",
    "    modelVars = {}\n",
    "    #print(\"here\")\n",
    "    modelVars['device'] = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(modelVars['device'])\n",
    "    # Def current CV set\n",
    "    mdlParams['trainInd'] = mdlParams['trainIndCV'][cv]\n",
    "    if 'valIndCV' in mdlParams:\n",
    "        mdlParams['valInd'] = mdlParams['valIndCV'][cv]\n",
    "    # Def current path for saving stuff\n",
    "    if 'valIndCV' in mdlParams:\n",
    "        mdlParams['saveDir'] = mdlParams['saveDirBase'] + '/CVSet' + str(cv)\n",
    "    else:\n",
    "        mdlParams['saveDir'] = mdlParams['saveDirBase']\n",
    "    # Create basepath if it doesnt exist yet\n",
    "    if not os.path.isdir(mdlParams['saveDirBase']):\n",
    "        os.mkdir(mdlParams['saveDirBase'])\n",
    "    # Check if there is something to load\n",
    "    load_old = 0\n",
    "    if os.path.isdir(mdlParams['saveDir']):\n",
    "        # Check if a checkpoint is in there\n",
    "        if len([name for name in os.listdir(mdlParams['saveDir'])]) > 0:\n",
    "            load_old = 1\n",
    "            print(\"Loading old model\")\n",
    "        else:\n",
    "            # Delete whatever is in there (nothing happens)\n",
    "            filelist = [os.remove(mdlParams['saveDir'] +'/'+f) for f in os.listdir(mdlParams['saveDir'])]\n",
    "    else:\n",
    "        os.mkdir(mdlParams['saveDir'])\n",
    "    # Save training progress in here\n",
    "    save_dict = {}\n",
    "    save_dict['acc'] = []\n",
    "    save_dict['loss'] = []\n",
    "    save_dict['wacc'] = []\n",
    "    save_dict['auc'] = []\n",
    "    save_dict['sens'] = []\n",
    "    save_dict['spec'] = []\n",
    "    save_dict['f1'] = []\n",
    "    save_dict['step_num'] = []\n",
    "    if mdlParams['print_trainerr']:\n",
    "        save_dict_train = {}\n",
    "        save_dict_train['acc'] = []\n",
    "        save_dict_train['loss'] = []\n",
    "        save_dict_train['wacc'] = []\n",
    "        save_dict_train['auc'] = []\n",
    "        save_dict_train['sens'] = []\n",
    "        save_dict_train['spec'] = []\n",
    "        save_dict_train['f1'] = []\n",
    "        save_dict_train['step_num'] = []        \n",
    "    # Potentially calculate setMean to subtract\n",
    "    if mdlParams['subtract_set_mean'] == 1:\n",
    "        mdlParams['setMean'] = np.mean(mdlParams['images_means'][mdlParams['trainInd'],:],(0))\n",
    "        print(\"Set Mean\",mdlParams['setMean']) \n",
    "\n",
    "    # balance classes\n",
    "    if mdlParams['balance_classes'] < 3 or mdlParams['balance_classes'] == 7 or mdlParams['balance_classes'] == 11:\n",
    "        # class_weights = class_weight.compute_class_weight('balanced',np.unique(np.argmax(mdlParams['labels_array'][mdlParams['trainInd'],:],1)),np.argmax(mdlParams['labels_array'][mdlParams['trainInd'],:],1)) \n",
    "        class_weights = class_weight.compute_class_weight(\n",
    "            class_weight='balanced',  # Use keyword argument\n",
    "            classes=np.unique(np.argmax(mdlParams['labels_array'][mdlParams['trainInd'], :], axis=1)),  # Unique classes\n",
    "            y=np.argmax(mdlParams['labels_array'][mdlParams['trainInd'], :], axis=1)  # Class labels\n",
    "        )\n",
    "        print(\"Current class weights\",class_weights)\n",
    "        class_weights = class_weights*mdlParams['extra_fac']\n",
    "        print(\"Current class weights with extra\",class_weights)             \n",
    "    elif mdlParams['balance_classes'] == 3 or mdlParams['balance_classes'] == 4:\n",
    "        # Split training set by classes\n",
    "        not_one_hot = np.argmax(mdlParams['labels_array'],1)\n",
    "        mdlParams['class_indices'] = []\n",
    "        for i in range(mdlParams['numClasses']):\n",
    "            mdlParams['class_indices'].append(np.where(not_one_hot==i)[0])\n",
    "            # Kick out non-trainind indices\n",
    "            mdlParams['class_indices'][i] = np.setdiff1d(mdlParams['class_indices'][i],mdlParams['valInd'])\n",
    "            #print(\"Class\",i,mdlParams['class_indices'][i].shape,np.min(mdlParams['class_indices'][i]),np.max(mdlParams['class_indices'][i]),np.sum(mdlParams['labels_array'][np.int64(mdlParams['class_indices'][i]),:],0))        \n",
    "    elif mdlParams['balance_classes'] == 5 or mdlParams['balance_classes'] == 6 or mdlParams['balance_classes'] == 13:\n",
    "        # Other class balancing loss\n",
    "        class_weights = 1.0/np.mean(mdlParams['labels_array'][mdlParams['trainInd'],:],axis=0)\n",
    "        print(\"Current class weights\",class_weights)\n",
    "        if isinstance(mdlParams['extra_fac'], float):\n",
    "            class_weights = np.power(class_weights,mdlParams['extra_fac'])\n",
    "        else:\n",
    "            class_weights = class_weights*mdlParams['extra_fac']\n",
    "        print(\"Current class weights with extra\",class_weights) \n",
    "    elif mdlParams['balance_classes'] == 9:\n",
    "        # Only use official indicies for calculation\n",
    "        print(\"Balance 9\")\n",
    "        indices_ham = mdlParams['trainInd'][mdlParams['trainInd'] < 25331]\n",
    "        if mdlParams['numClasses'] == 9:\n",
    "            class_weights_ = 1.0/np.mean(mdlParams['labels_array'][indices_ham,:8],axis=0)\n",
    "            #print(\"class before\",class_weights_)\n",
    "            class_weights = np.zeros([mdlParams['numClasses']])\n",
    "            class_weights[:8] = class_weights_\n",
    "            class_weights[-1] = np.max(class_weights_)\n",
    "        else:\n",
    "            class_weights = 1.0/np.mean(mdlParams['labels_array'][indices_ham,:],axis=0)\n",
    "        print(\"Current class weights\",class_weights)             \n",
    "        if isinstance(mdlParams['extra_fac'], float):\n",
    "            class_weights = np.power(class_weights,mdlParams['extra_fac'])\n",
    "        else:\n",
    "            class_weights = class_weights*mdlParams['extra_fac']\n",
    "        print(\"Current class weights with extra\",class_weights)             \n",
    "\n",
    "    # Meta scaler\n",
    "    if mdlParams.get('meta_features',None) is not None and mdlParams['scale_features']:\n",
    "        mdlParams['feature_scaler_meta'] = sklearn.preprocessing.StandardScaler().fit(mdlParams['meta_array'][mdlParams['trainInd'],:])  \n",
    "        print(\"scaler mean\",mdlParams['feature_scaler_meta'].mean_,\"var\",mdlParams['feature_scaler_meta'].var_)  \n",
    "\n",
    "    # Set up dataloaders\n",
    "    num_workers = psutil.cpu_count(logical=False)\n",
    "    # For train\n",
    "    dataset_train = ISICDataset(mdlParams, 'trainInd')\n",
    "    # For val\n",
    "    dataset_val = ISICDataset(mdlParams, 'valInd')\n",
    "    if mdlParams['multiCropEval'] > 0:\n",
    "        modelVars['dataloader_valInd'] = DataLoader(dataset_val, batch_size=mdlParams['multiCropEval'], shuffle=False, num_workers=num_workers, pin_memory=True)  \n",
    "    else:\n",
    "        modelVars['dataloader_valInd'] = DataLoader(dataset_val, batch_size=mdlParams['batchSize'], shuffle=False, num_workers=num_workers, pin_memory=True)               \n",
    "\n",
    "    if mdlParams['balance_classes'] == 12 or mdlParams['balance_classes'] == 13:\n",
    "        #print(np.argmax(mdlParams['labels_array'][mdlParams['trainInd'],:],1).size(0))\n",
    "        strat_sampler = StratifiedSampler(mdlParams)\n",
    "        modelVars['dataloader_trainInd'] = DataLoader(dataset_train, batch_size=mdlParams['batchSize'], sampler=strat_sampler, num_workers=num_workers, pin_memory=True) \n",
    "    else:\n",
    "        modelVars['dataloader_trainInd'] = DataLoader(dataset_train, batch_size=mdlParams['batchSize'], shuffle=True, num_workers=num_workers, pin_memory=True, drop_last=True) \n",
    "    #print(\"Setdiff\",np.setdiff1d(mdlParams['trainInd'],mdlParams['trainInd']))\n",
    "    # Define model \n",
    "    modelVars['model'] = getModel(mdlParams)()  \n",
    "    # Load trained model\n",
    "    if mdlParams.get('meta_features',None) is not None:\n",
    "        # Find best checkpoint\n",
    "        files = glob(mdlParams['model_load_path'] + '/CVSet' + str(cv) + '/*')\n",
    "        global_steps = np.zeros([len(files)])\n",
    "        #print(\"files\",files)\n",
    "        for i in range(len(files)):\n",
    "            # Use meta files to find the highest index\n",
    "            if 'best' not in files[i]:\n",
    "                continue\n",
    "            if 'checkpoint' not in files[i]:\n",
    "                continue                \n",
    "            # Extract global step\n",
    "            nums = [int(s) for s in re.findall(r'\\d+',files[i])]\n",
    "            global_steps[i] = nums[-1]\n",
    "        # Create path with maximum global step found\n",
    "        chkPath = mdlParams['model_load_path'] + '/CVSet' + str(cv) + '/checkpoint_best-' + str(int(np.max(global_steps))) + '.pt'\n",
    "        print(\"Restoring lesion-trained CNN for meta data training: \",chkPath)\n",
    "        # Load\n",
    "        state = torch.load(chkPath)\n",
    "        # Initialize model\n",
    "        curr_model_dict = modelVars['model'].state_dict()\n",
    "        for name, param in state['state_dict'].items():\n",
    "            #print(name,param.shape)\n",
    "            if isinstance(param, nn.Parameter):\n",
    "                # backwards compatibility for serialized parameters\n",
    "                param = param.data\n",
    "            if curr_model_dict[name].shape == param.shape:\n",
    "                curr_model_dict[name].copy_(param)\n",
    "            else:\n",
    "                print(\"not restored\",name,param.shape)\n",
    "        #modelVars['model'].load_state_dict(state['state_dict'])        \n",
    "    # Original input size\n",
    "    #if 'Dense' not in mdlParams['model_type']:\n",
    "    #    print(\"Original input size\",modelVars['model'].input_size)\n",
    "    #print(modelVars['model'])\n",
    "    if 'Dense' in mdlParams['model_type']:\n",
    "        if mdlParams['input_size'][0] != 224:\n",
    "            modelVars['model'] = modify_densenet_avg_pool(modelVars['model'])\n",
    "            #print(modelVars['model'])\n",
    "        num_ftrs = modelVars['model'].classifier.in_features\n",
    "        modelVars['model'].classifier = nn.Linear(num_ftrs, mdlParams['numClasses'])\n",
    "        #print(modelVars['model'])\n",
    "    elif 'dpn' in mdlParams['model_type']:\n",
    "        num_ftrs = modelVars['model'].classifier.in_channels\n",
    "        modelVars['model'].classifier = nn.Conv2d(num_ftrs,mdlParams['numClasses'],[1,1])\n",
    "        #modelVars['model'].add_module('real_classifier',nn.Linear(num_ftrs, mdlParams['numClasses']))\n",
    "        #print(modelVars['model'])\n",
    "    elif 'efficient' in mdlParams['model_type']:\n",
    "        # Do nothing, output is prepared\n",
    "        num_ftrs = modelVars['model']._fc.in_features\n",
    "        modelVars['model']._fc = nn.Linear(num_ftrs, mdlParams['numClasses'])    \n",
    "    elif 'wsl' in mdlParams['model_type']:\n",
    "        num_ftrs = modelVars['model'].fc.in_features\n",
    "        modelVars['model'].fc = nn.Linear(num_ftrs, mdlParams['numClasses'])          \n",
    "    else:\n",
    "        num_ftrs = modelVars['model'].last_linear.in_features\n",
    "        modelVars['model'].last_linear = nn.Linear(num_ftrs, mdlParams['numClasses'])    \n",
    "    # Take care of meta case\n",
    "    if mdlParams.get('meta_features',None) is not None:\n",
    "        # freeze cnn first\n",
    "        if mdlParams['freeze_cnn']:\n",
    "            # deactivate all\n",
    "            for param in modelVars['model'].parameters():\n",
    "                param.requires_grad = False            \n",
    "            if 'efficient' in mdlParams['model_type']:\n",
    "                # Activate fc\n",
    "                for param in modelVars['model']._fc.parameters():\n",
    "                    param.requires_grad = True\n",
    "            elif 'wsl' in mdlParams['model_type']:\n",
    "                # Activate fc\n",
    "                for param in modelVars['model'].fc.parameters():\n",
    "                    param.requires_grad = True\n",
    "            else:\n",
    "                # Activate fc\n",
    "                for param in modelVars['model'].last_linear.parameters():\n",
    "                    param.requires_grad = True                                \n",
    "        else:\n",
    "            # mark cnn parameters\n",
    "            for param in modelVars['model'].parameters():\n",
    "                param.is_cnn_param = True\n",
    "            # unmark fc\n",
    "            for param in modelVars['model']._fc.parameters():\n",
    "                param.is_cnn_param = False                              \n",
    "        # modify model\n",
    "        print(mdlParams)\n",
    "        modelVars['model'] = models.modify_meta(mdlParams,modelVars['model'])  \n",
    "        # Mark new parameters\n",
    "        for param in modelVars['model'].parameters():\n",
    "            if not hasattr(param, 'is_cnn_param'):\n",
    "                param.is_cnn_param = False                 \n",
    "    # multi gpu support\n",
    "    if len(mdlParams['numGPUs']) > 1:\n",
    "        modelVars['model'] = nn.DataParallel(modelVars['model']) \n",
    "    modelVars['model'] = modelVars['model'].cuda()\n",
    "    #summary(modelVars['model'], modelVars['model'].input_size)# (mdlParams['input_size'][2], mdlParams['input_size'][0], mdlParams['input_size'][1]))\n",
    "    # Loss, with class weighting\n",
    "    if mdlParams.get('focal_loss',False):\n",
    "        modelVars['criterion'] = FocalLoss(alpha=class_weights.tolist())\n",
    "    elif mdlParams['balance_classes'] == 3 or mdlParams['balance_classes'] == 0 or mdlParams['balance_classes'] == 12:\n",
    "        modelVars['criterion'] = nn.CrossEntropyLoss()\n",
    "    elif mdlParams['balance_classes'] == 8:\n",
    "        modelVars['criterion'] = nn.CrossEntropyLoss(reduce=False)\n",
    "    elif mdlParams['balance_classes'] == 6 or mdlParams['balance_classes'] == 7:\n",
    "        modelVars['criterion'] = nn.CrossEntropyLoss(weight=torch.cuda.FloatTensor(class_weights.astype(np.float32)),reduce=False)\n",
    "    elif mdlParams['balance_classes'] == 10:\n",
    "        modelVars['criterion'] = FocalLoss(mdlParams['numClasses'])\n",
    "    elif mdlParams['balance_classes'] == 11:\n",
    "        modelVars['criterion'] = FocalLoss(mdlParams['numClasses'],alpha=torch.cuda.FloatTensor(class_weights.astype(np.float32)))\n",
    "    else:\n",
    "        modelVars['criterion'] = nn.CrossEntropyLoss(weight=torch.cuda.FloatTensor(class_weights.astype(np.float32)))\n",
    "\n",
    "    if mdlParams.get('meta_features',None) is not None:\n",
    "        if mdlParams['freeze_cnn']:\n",
    "            modelVars['optimizer'] = optim.Adam(filter(lambda p: p.requires_grad, modelVars['model'].parameters()), lr=mdlParams['learning_rate_meta'])\n",
    "            # sanity check\n",
    "            for param in filter(lambda p: p.requires_grad, modelVars['model'].parameters()):\n",
    "                print(param.name,param.shape)\n",
    "        else:\n",
    "            modelVars['optimizer'] = optim.Adam([\n",
    "                                                {'params': filter(lambda p: not p.is_cnn_param, modelVars['model'].parameters()), 'lr': mdlParams['learning_rate_meta']},\n",
    "                                                {'params': filter(lambda p: p.is_cnn_param, modelVars['model'].parameters()), 'lr': mdlParams['learning_rate']}\n",
    "                                                ], lr=mdlParams['learning_rate'])\n",
    "    else:\n",
    "        modelVars['optimizer'] = optim.Adam(modelVars['model'].parameters(), lr=mdlParams['learning_rate'])\n",
    "\n",
    "    # Decay LR by a factor of 0.1 every 7 epochs\n",
    "    modelVars['scheduler'] = lr_scheduler.StepLR(modelVars['optimizer'], step_size=mdlParams['lowerLRAfter'], gamma=1/np.float32(mdlParams['LRstep']))\n",
    "\n",
    "    # Define softmax\n",
    "    modelVars['softmax'] = nn.Softmax(dim=1)\n",
    "\n",
    "    \n",
    "    # Set up training\n",
    "    # loading from checkpoint\n",
    "    if load_old:\n",
    "        # Find last, not last best checkpoint\n",
    "        files = glob(mdlParams['saveDir']+'/*')\n",
    "        global_steps = np.zeros([len(files)])\n",
    "        for i in range(len(files)):\n",
    "            # Use meta files to find the highest index\n",
    "            if 'best' in files[i]:\n",
    "                continue\n",
    "            if 'checkpoint-' not in files[i]:\n",
    "                continue                \n",
    "            # Extract global step\n",
    "            nums = [int(s) for s in re.findall(r'\\d+',files[i])]\n",
    "            global_steps[i] = nums[-1]\n",
    "        # Create path with maximum global step found\n",
    "        chkPath = mdlParams['saveDir'] + '/checkpoint-' + str(int(np.max(global_steps))) + '.pt'\n",
    "        print(\"Restoring: \",chkPath)\n",
    "        # Load\n",
    "        state = torch.load(chkPath)\n",
    "        # Initialize model and optimizer\n",
    "        modelVars['model'].load_state_dict(state['state_dict'])\n",
    "        modelVars['optimizer'].load_state_dict(state['optimizer'])     \n",
    "        start_epoch = state['epoch']+1\n",
    "        mdlParams['valBest'] = state.get('valBest',1000)\n",
    "        mdlParams['lastBestInd'] = state.get('lastBestInd',int(np.max(global_steps)))\n",
    "    else:\n",
    "        start_epoch = 1\n",
    "        mdlParams['lastBestInd'] = -1\n",
    "        # Track metrics for saving best model\n",
    "        mdlParams['valBest'] = 1000\n",
    "\n",
    "    # Num batches\n",
    "    numBatchesTrain = int(math.floor(len(mdlParams['trainInd'])/mdlParams['batchSize']))\n",
    "    print(\"Train batches\",numBatchesTrain)\n",
    "\n",
    "    # Run training\n",
    "    start_time = time.time()\n",
    "    print(\"Start training...\")\n",
    "    for step in range(start_epoch, mdlParams['training_steps']+1):\n",
    "        # One Epoch of training\n",
    "        if step >= mdlParams['lowerLRat']-mdlParams['lowerLRAfter']:\n",
    "            modelVars['scheduler'].step()\n",
    "        modelVars['model'].train()      \n",
    "        for j, (inputs, labels, indices) in enumerate(modelVars['dataloader_trainInd']):    \n",
    "            #print(indices)                  \n",
    "            #t_load = time.time() \n",
    "            # Run optimization        \n",
    "            if mdlParams.get('meta_features',None) is not None: \n",
    "                inputs[0] = inputs[0].cuda()\n",
    "                inputs[1] = inputs[1].cuda()\n",
    "            else:\n",
    "                inputs = inputs.cuda()\n",
    "            #print(inputs.shape)\n",
    "            labels = labels.cuda()        \n",
    "            # zero the parameter gradients\n",
    "            modelVars['optimizer'].zero_grad()             \n",
    "            # forward\n",
    "            # track history if only in train\n",
    "            with torch.set_grad_enabled(True):             \n",
    "                if mdlParams.get('aux_classifier',False):\n",
    "                    outputs, outputs_aux = modelVars['model'](inputs) \n",
    "                    loss1 = modelVars['criterion'](outputs, labels)\n",
    "                    labels_aux = labels.repeat(mdlParams['multiCropTrain'])\n",
    "                    loss2 = modelVars['criterion'](outputs_aux, labels_aux) \n",
    "                    loss = loss1 + mdlParams['aux_classifier_loss_fac']*loss2     \n",
    "                else:               \n",
    "                    #print(\"load\",time.time()-t_load)    \n",
    "                    #t_fwd = time.time()   \n",
    "                    outputs = modelVars['model'](inputs)     \n",
    "                    #print(\"forward\",time.time()-t_fwd)     \n",
    "                    #t_bwd = time.time()   \n",
    "                    loss = modelVars['criterion'](outputs, labels)         \n",
    "                # Perhaps adjust weighting of the loss by the specific index\n",
    "                if mdlParams['balance_classes'] == 6 or mdlParams['balance_classes'] == 7 or mdlParams['balance_classes'] == 8:\n",
    "                    #loss = loss.cpu()\n",
    "                    indices = indices.numpy()\n",
    "                    loss = loss*torch.cuda.FloatTensor(mdlParams['loss_fac_per_example'][indices].astype(np.float32))\n",
    "                    loss = torch.mean(loss)\n",
    "                    #loss = loss.cuda()\n",
    "                # backward + optimize only if in training phase\n",
    "                loss.backward()                 \n",
    "                modelVars['optimizer'].step()     \n",
    "                #print(\"backward\",time.time()-t_bwd)                             \n",
    "        if step % mdlParams['display_step'] == 0 or step == 1:\n",
    "            # Calculate evaluation metrics\n",
    "            if mdlParams['classification']:\n",
    "                # Adjust model state\n",
    "                modelVars['model'].eval()\n",
    "                # Get metrics\n",
    "                loss, accuracy, sensitivity, specificity, conf_matrix, f1, auc, waccuracy, predictions, targets, _ = getErrClassification_mgpu(mdlParams, eval_set, modelVars)\n",
    "                # Save in mat\n",
    "                save_dict['loss'].append(loss)\n",
    "                save_dict['acc'].append(accuracy)\n",
    "                save_dict['wacc'].append(waccuracy)\n",
    "                save_dict['auc'].append(auc)\n",
    "                save_dict['sens'].append(sensitivity)\n",
    "                save_dict['spec'].append(specificity)\n",
    "                save_dict['f1'].append(f1)\n",
    "                save_dict['step_num'].append(step)\n",
    "                if os.path.isfile(mdlParams['saveDir'] + '/progression_'+eval_set+'.mat'):\n",
    "                    os.remove(mdlParams['saveDir'] + '/progression_'+eval_set+'.mat')                \n",
    "                io.savemat(mdlParams['saveDir'] + '/progression_'+eval_set+'.mat',save_dict)                \n",
    "            eval_metric = -np.mean(waccuracy)\n",
    "            # Check if we have a new best value\n",
    "            if eval_metric < mdlParams['valBest']:\n",
    "                mdlParams['valBest'] = eval_metric\n",
    "                if mdlParams['classification']:\n",
    "                    allData['f1Best'][cv] = f1\n",
    "                    allData['sensBest'][cv] = sensitivity\n",
    "                    allData['specBest'][cv] = specificity\n",
    "                    allData['accBest'][cv] = accuracy\n",
    "                    allData['waccBest'][cv] = waccuracy\n",
    "                    allData['aucBest'][cv] = auc\n",
    "                oldBestInd = mdlParams['lastBestInd']\n",
    "                mdlParams['lastBestInd'] = step\n",
    "                allData['convergeTime'][cv] = step\n",
    "                # Save best predictions\n",
    "                allData['bestPred'][cv] = predictions\n",
    "                allData['targets'][cv] = targets\n",
    "                # Write to File\n",
    "                with open(mdlParams['saveDirBase'] + '/CV.pkl', 'wb') as f:\n",
    "                    pickle.dump(allData, f, pickle.HIGHEST_PROTOCOL)                 \n",
    "                # Delte previously best model\n",
    "                if os.path.isfile(mdlParams['saveDir'] + '/checkpoint_best-' + str(oldBestInd) + '.pt'):\n",
    "                    os.remove(mdlParams['saveDir'] + '/checkpoint_best-' + str(oldBestInd) + '.pt')\n",
    "                # Save currently best model\n",
    "                state = {'epoch': step, 'valBest': mdlParams['valBest'], 'lastBestInd': mdlParams['lastBestInd'], 'state_dict': modelVars['model'].state_dict(),'optimizer': modelVars['optimizer'].state_dict()}\n",
    "                torch.save(state, mdlParams['saveDir'] + '/checkpoint_best-' + str(step) + '.pt')               \n",
    "                            \n",
    "            # If its not better, just save it delete the last checkpoint if it is not current best one\n",
    "            # Save current model\n",
    "            state = {'epoch': step, 'valBest': mdlParams['valBest'], 'lastBestInd': mdlParams['lastBestInd'], 'state_dict': modelVars['model'].state_dict(),'optimizer': modelVars['optimizer'].state_dict()}\n",
    "            torch.save(state, mdlParams['saveDir'] + '/checkpoint-' + str(step) + '.pt')                           \n",
    "            # Delete last one\n",
    "            if step == mdlParams['display_step']:\n",
    "                lastInd = 1\n",
    "            else:\n",
    "                lastInd = step-mdlParams['display_step']\n",
    "            if os.path.isfile(mdlParams['saveDir'] + '/checkpoint-' + str(lastInd) + '.pt'):\n",
    "                os.remove(mdlParams['saveDir'] + '/checkpoint-' + str(lastInd) + '.pt')       \n",
    "            # Duration so far\n",
    "            duration = time.time() - start_time                          \n",
    "            # Print\n",
    "            if mdlParams['classification']:\n",
    "                print(\"\\n\")\n",
    "                print(\"Config:\",sys.argv[2])\n",
    "                print('Fold: %d Epoch: %d/%d (%d h %d m %d s)' % (cv,step,mdlParams['training_steps'], int(duration/3600), int(np.mod(duration,3600)/60), int(np.mod(np.mod(duration,3600),60))) + time.strftime(\"%d.%m.-%H:%M:%S\", time.localtime()))\n",
    "                print(\"Loss on \",eval_set,\"set: \",loss,\" Accuracy: \",accuracy,\" F1: \",f1,\" (best WACC: \",-mdlParams['valBest'],\" at Epoch \",mdlParams['lastBestInd'],\")\")\n",
    "                print(\"Auc\",auc,\"Mean AUC\",np.mean(auc))\n",
    "                print(\"Per Class Acc\",waccuracy,\"Weighted Accuracy\",np.mean(waccuracy))\n",
    "                print(\"Sensitivity: \",sensitivity,\"Specificity\",specificity)\n",
    "                print(\"Confusion Matrix\")\n",
    "                print(conf_matrix)\n",
    "                # Potentially peek at test error\n",
    "                if mdlParams['peak_at_testerr']:              \n",
    "                    loss, accuracy, sensitivity, specificity, _, f1, _, _, _, _, _ = getErrClassification_mgpu(mdlParams, 'testInd', modelVars)\n",
    "                    print(\"Test loss: \",loss,\" Accuracy: \",accuracy,\" F1: \",f1)\n",
    "                    print(\"Sensitivity: \",sensitivity,\"Specificity\",specificity)\n",
    "                # Potentially print train err\n",
    "                if mdlParams['print_trainerr'] and 'train' not in eval_set:                \n",
    "                    loss, accuracy, sensitivity, specificity, conf_matrix, f1, auc, waccuracy, predictions, targets, _ = getErrClassification_mgpu(mdlParams, 'trainInd', modelVars)\n",
    "                    # Save in mat\n",
    "                    save_dict_train['loss'].append(loss)\n",
    "                    save_dict_train['acc'].append(accuracy)\n",
    "                    save_dict_train['wacc'].append(waccuracy)\n",
    "                    save_dict_train['auc'].append(auc)\n",
    "                    save_dict_train['sens'].append(sensitivity)\n",
    "                    save_dict_train['spec'].append(specificity)\n",
    "                    save_dict_train['f1'].append(f1)\n",
    "                    save_dict_train['step_num'].append(step)\n",
    "                    if os.path.isfile(mdlParams['saveDir'] + '/progression_trainInd.mat'):\n",
    "                        os.remove(mdlParams['saveDir'] + '/progression_trainInd.mat')                \n",
    "                    scipy.io.savemat(mdlParams['saveDir'] + '/progression_trainInd.mat',save_dict_train)                     \n",
    "                    print(\"Train loss: \",loss,\" Accuracy: \",accuracy,\" F1: \",f1)\n",
    "                    print(\"Sensitivity: \",sensitivity,\"Specificity\",specificity)\n",
    "    # Free everything in modelvars\n",
    "    modelVars.clear()\n",
    "    # After CV Training: print CV results and save them\n",
    "    print(\"Best F1:\",allData['f1Best'][cv])\n",
    "    print(\"Best Sens:\",allData['sensBest'][cv])\n",
    "    print(\"Best Spec:\",allData['specBest'][cv])\n",
    "    print(\"Best Acc:\",allData['accBest'][cv])\n",
    "    print(\"Best Per Class Accuracy:\",allData['waccBest'][cv])\n",
    "    print(\"Best Weighted Acc:\",np.mean(allData['waccBest'][cv]))\n",
    "    print(\"Best AUC:\",allData['aucBest'][cv])\n",
    "    print(\"Best Mean AUC:\",np.mean(allData['aucBest'][cv]))    \n",
    "    print(\"Convergence Steps:\",allData['convergeTime'][cv])"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "databundleVersionId": 9094797,
     "sourceId": 63056,
     "sourceType": "competition"
    },
    {
     "datasetId": 679322,
     "sourceId": 1193409,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6171362,
     "sourceId": 10032440,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6152590,
     "sourceId": 10043023,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6124691,
     "sourceId": 10055204,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30301,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
